<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Qian's Blog - misc</title><link href="https://qian-gu.github.io/" rel="alternate"></link><link href="https://qian-gu.github.io/feeds/misc.atom.xml" rel="self"></link><id>https://qian-gu.github.io/</id><updated>2025-10-11T23:30:00+08:00</updated><subtitle>Read &gt;&gt; Think &gt;&gt; Write</subtitle><entry><title>《Attention Is All You Need》 笔记</title><link href="https://qian-gu.github.io/posts/misc/attention-is-all-you-need-bi-ji.html" rel="alternate"></link><published>2025-10-11T23:30:00+08:00</published><updated>2025-10-11T23:30:00+08:00</updated><author><name>Qian Gu</name></author><id>tag:qian-gu.github.io,2025-10-11:/posts/misc/attention-is-all-you-need-bi-ji.html</id><summary type="html">&lt;p&gt;Transformer 论文笔记。&lt;/p&gt;</summary><content type="html">&lt;div class="toc"&gt;&lt;span class="toctitle"&gt;Table of Contents&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#backgroud"&gt;Backgroud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#model-architecture"&gt;Model Architecture&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#encoder-and-decoder-stack"&gt;encoder and decoder stack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#attention"&gt;Attention&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#scaled-dot-product-attention"&gt;Scaled Dot-Product Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#multi-head-attention"&gt;Multi-Head Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#multi-head-attention-in-model"&gt;Multi-head Attention in Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#point-wise-feed-forward-networks"&gt;Point-wise Feed-Forward Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#embedding-and-softmax"&gt;Embedding and Softmax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#positional-embedding"&gt;Positional Embedding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#why-self-attention"&gt;Why Self-Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#result"&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ref"&gt;Ref&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景知识：encoder-decoder&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;encoder-decoder 模型是 NLP 领域的概念，不是指具体的算法，而是一类算法的统称，是一个通用框架，在这个框架下可以使用不同算法解决不同任务。&lt;/p&gt;
&lt;p&gt;encoder 的作用：将现实问题转化为数学问题。&lt;/p&gt;
&lt;p&gt;&lt;img alt="encoder" src="/images/attention-is-all-you-need-note/encoder.png.webp"&gt;&lt;/p&gt;
&lt;p&gt;decoder 的作用：将数学问题转化为现实问题。&lt;/p&gt;
&lt;p&gt;&lt;img alt="decoder" src="/images/attention-is-all-you-need-note/decoder.png.webp"&gt;&lt;/p&gt;
&lt;p&gt;两个连在一起：&lt;/p&gt;
&lt;p&gt;&lt;img alt="encoder-decoder" src="/images/attention-is-all-you-need-note/encoder-decoder.png.webp"&gt;&lt;/p&gt;
&lt;p&gt;需要注意的两点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;无论输入输出的长度是多少，中间向量 C 的长度固定，显然长序列会有数据损失。&lt;/li&gt;
&lt;li&gt;根据任务不同，encoder 和 decoder 可以用不同的视线，如 RNN，LSTM 或 GRU 等。&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景知识：seq2seq&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;seq2seq 如字面意思，输入一个 sequence，输出一个 sequence，重点在于 sequence 的长度是可变的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="nmt-model-fast" src="/images/attention-is-all-you-need-note/nmt-model-fast.gif"&gt;&lt;/p&gt;
&lt;p&gt;典型的序列转换模型通常包括一个编码器（encoder）和一个解码器（decoder）。encoder 负责将输入序列编码成一个固定长度的隐状态表示，而 decoder 则利用这个隐状态表示生成目标序列。&lt;/p&gt;
&lt;p&gt;在这些模型中，循环神经网络（RNN）和卷积神经网络（CNN）是从前最常见的架构。然而，2017 年以来，基于注意力机制的 Transformer 架构（即本论文介绍的架构）因其并行计算能力和处理长距离依赖关系的优势，成为序列转换任务中的新宠。&lt;/p&gt;
&lt;p&gt;在 transformer 之前，主流方法是 RNN 或者 CNN，但是两者都有各自的缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNN 通过隐藏状态可以记住所有历史，但是隐藏状态只能递归计算，无法并行化；&lt;/li&gt;
&lt;li&gt;CNN 可以并行计算，但是对长距离 token 之间相关性的建模能力很弱。它只能对 kernel size 内的 token 之间的相关性建模，如果需要建模长距离相关性，则必须级联很多层；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;transformer 试图结合两者的优点：既能建模长距离，又能并行化。&lt;/p&gt;
&lt;p&gt;Q：seq2seq 和 encoder-decoder 的区别：&lt;/p&gt;
&lt;p&gt;A：seq2seq 强调目的，encoder-decoder 强调方法，seq2seq 使用的方法基本上都属于 encoder-decoder 模型。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景知识：embedding&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;计算机要处理任何信息都必须先将其转化成数值，比如人类可以理解的单词，但是因为每个单词有多重属性，只转化出 1 个值能表达信息的能力有限，所以一般会转成多个值，这些值组合在一起形成 1 个向量，即该 token 的向量表示 &lt;code&gt;embedding vector&lt;/code&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景知识：attention&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;encoder-docoder 架构中的中间向量 C 是固定长度，所以对于长序列，压缩后的会有信息丢失，算法效果不好。attention 机制中的 encoder 编码完的结果不再是一个固定长度的中间向量 C，而是一个向量序列，这样就能解决这个问题。&lt;/p&gt;
&lt;p&gt;&lt;img alt="attention" src="/images/attention-is-all-you-need-note/attention-arch.png.webp"&gt;&lt;/p&gt;
&lt;p&gt;主要有两个特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;没有信息丢失。&lt;/li&gt;
&lt;li&gt;重要 token 和次要 token 权重不同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;attention 机制和人类处理信息的方式类似：优先关注重要信息。我们阅读一段文字时，不会平均分配注意力到每个字上面，而是会重点阅读和问题（query）相关的文字（key 和 value）。&lt;/p&gt;
&lt;p&gt;attention 通过计算 query 和 key 的相似度，动态地调整对 input 的关注程度，所以能更有效地处理复杂任务。在数学上 attention 机制就是对 Source 中的元素的 Value 加权求和，而 Query 和 Key 用来计算对应 Value 的权重系数：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Attention(Query, Source) = \sum_{i=1}^{L_x}a_i * Value_i\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(L_x\)&lt;/span&gt; 表示序列的长度。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(a_i = sofmax(Sim_i) = \frac{e^{Sim_i}}{\sum_{j=1}^{L_x}e^{Sim_j}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因为两个 token 对应两个向量，所以衡量两个 token 之间的相似度 &lt;span class="math"&gt;\(Sim_i\)&lt;/span&gt; 也就变成了衡量两个向量之间的相似度。一般使用的方法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cos 相似度：&lt;span class="math"&gt;\(s(q, k) = \frac{q^T k}{\lvert q \rvert \cdot \lvert k \rvert}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;向量点积：&lt;span class="math"&gt;\(s(q, k) = q^T k\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;attention 一般分为下图的 3 个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一步，query 和 key 计算相似度，得到打分 score；&lt;/li&gt;
&lt;li&gt;第二步，将 score 归一化，得到每个 value 的 weight；&lt;/li&gt;
&lt;li&gt;第三步，用 weight 对 value 加权求和；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="attention" src="/images/attention-is-all-you-need-note/attention.png"&gt;&lt;/p&gt;
&lt;p&gt;注意：上图中的 query，key，value 可以是标量，也可以是向量。&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RNN、LSTM、特别是 Gated Recurrent Neural Network 在语言建模和机器翻译等序列建模和转换任务中已经是公认的 SOTA 方法。业界有很多方法持续扩展递归语言模型和 encoder-decoder 架构的能力边界。&lt;/li&gt;
&lt;li&gt;循环模型一般沿着输入和输出进行计算，根据前一隐藏状态 &lt;span class="math"&gt;\(h_{t-1}\)&lt;/span&gt; 和当前位置 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 计算当前隐藏状态 &lt;span class="math"&gt;\(h_t\)&lt;/span&gt;，&lt;strong&gt;这种递归顺序阻碍了训练的并行化。特别是在长序列中更加明显，因为有限的内存阻碍了跨样本的 batch 并行处理。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;尽管通过 factorization trick 和 conditional computation 可以大幅提高递归模型的计算效率，在某种情况下还提高了模型性能，但是递归的顺序计算约束仍然存在。&lt;/li&gt;
&lt;li&gt;在各种任务中，attention 机制已经成为序列模型和转换任务重不可或缺的组成部分，因为它可以在不考虑 sequence 中 token 的距离的情况下建立依赖关系。但是除了少数情况，大部分情况下 attention 都和循环网络结合使用。&lt;/li&gt;
&lt;li&gt;本文提出一种名为 &lt;code&gt;transformer&lt;/code&gt; 的新模型架构，完全抛弃了递归网络，只依赖 attention 机制捕捉输入输出之间的全局相关性。这种架构可以有更高的并行度，只需要在 8 个 P100 上训练 12 个小时就能在翻译任务上达到 SOTA。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="backgroud"&gt;Backgroud&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;减少顺序计算的需求也催生了 ByteNet 和 CONVS2S 等模型，它们都采用 CNN 作为基本 block，并行计算所有输入、输出之间的 hidden representation。&lt;/li&gt;
&lt;li&gt;但是这些模型中，在任意两个输入、输出位置之间建立相关性需要的操作数，会随着位置距离的增加而增加，在 ByteNet 中线性关系，在 CONVS2S 中为对数关系。所以学习远距离之间的依赖变得很困难。&lt;/li&gt;
&lt;li&gt;transformer 可以将这个复杂度降低为常数，但是代价是平均注意力 weight 可能会降低有效分辨率，我们通过 multihead attention 机制来克服这个问题。&lt;/li&gt;
&lt;li&gt;self-attention 是一种通过序列不同位置之间的相关性来计算序列 representation 的 attention 机制，应用在很多任务中。&lt;/li&gt;
&lt;li&gt;transformer 是第一个只依赖 attention 机制的模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="model-architecture"&gt;Model Architecture&lt;/h2&gt;
&lt;p&gt;大部分序列转换 model 都基于 encoder-decoder 架构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;encoder 把输入序列 &lt;span class="math"&gt;\((x_1, x_2, \dots, x_n)\)&lt;/span&gt; 转化为一个连续的向量表示 &lt;span class="math"&gt;\(z = (z_1, z_2, \dots, z_n)\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;给定 &lt;span class="math"&gt;\(z\)&lt;/span&gt;，decoder 以一次生成 1 个字符的方式生成输出序列 &lt;span class="math"&gt;\((y_1, y_2, \dots, y_n)\)&lt;/span&gt;，每一步都是自回归的，即每次都会将之前的输出也作为输入的一部分；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;transformer 也遵循这样的结构，encoder 和 decoder 都基于堆叠的 self-attention 和 point-wise fc 层。&lt;/p&gt;
&lt;p&gt;&lt;img alt="model-arch" src="/images/attention-is-all-you-need-note/model-arch.png"&gt;&lt;/p&gt;
&lt;h3 id="encoder-and-decoder-stack"&gt;encoder and decoder stack&lt;/h3&gt;
&lt;p&gt;encoder 由 N = 6 个完全相同的 layer 堆叠组成，每个 layer 由 2 个 sub-layer 组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一个 sub-layer 是 multi-head self-attention；&lt;/li&gt;
&lt;li&gt;第二个 sub-layer 是一个简单的 point-wise fc 前馈网络；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每个 sub-layer 的输出都采用 residual 连接后接一个 layer norm 层，最终输出为 &lt;span class="math"&gt;\(LayerNorm(x + sublayer(x))\)&lt;/span&gt;，其中 &lt;span class="math"&gt;\(sublayer(x)\)&lt;/span&gt; 为 sub-layer 本身的函数功能。&lt;/p&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;residual 的目的是防止网络退化；&lt;/li&gt;
&lt;li&gt;layer norm 的目的是对每一层的 activation 进行归一化；&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;为了实现 residual 连接，所有 sub-layer，包括 embedding 在内，输出维度均为 &lt;span class="math"&gt;\(d_{model} = 512\)&lt;/span&gt;，一般设置为训练时的最长 sequence 的 token 数量。&lt;/p&gt;
&lt;p&gt;decoder 也由 N = 6 个完全相同的 layer 堆叠组成，除了 encoder 中的两个 sub-layer 外，decoder 中还额外插入了第三个 sub-layer，该 sub-layer 对 encoder output 做 multi-head attention 处理。 &lt;/p&gt;
&lt;p&gt;和 encoder 类似，decoder 的每个 sub-layer 也使用 residual + layer norm 连接。此外 decoder 中的 self-attention 还做了特殊设计，以防止后续位置的信息被添加到当前位置信息中，这种 mask 机制和 embedding 的偏移机制相结合，可以确保位置 i 只依赖小于 i 的已知输出，保证了 decoder 的自回归性。&lt;/p&gt;
&lt;h3 id="attention"&gt;Attention&lt;/h3&gt;
&lt;p&gt;attention 机制是一种将 query 和 1 组 key-value pair 映射为 output 的过程，其中 query，keys, values，output 都是 vector。&lt;/p&gt;
&lt;p&gt;output 是 values 的加权求和，每个 value 的权重由 query 和对应的 key 的 competibility function 计算得到。&lt;/p&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;本文用到 competibility function 是 dot-product。&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id="scaled-dot-product-attention"&gt;Scaled Dot-Product Attention&lt;/h4&gt;
&lt;p&gt;&lt;img alt="attention" src="/images/attention-is-all-you-need-note/multi-head-attention.png"&gt;&lt;/p&gt;
&lt;p&gt;本文的 attention 叫做 scaled dot-product attention：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入为 queries 和 keys，维度均为 &lt;span class="math"&gt;\(d_k\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;计算 queries 和所有 keys 之间的点乘计算相关性，然后除以 &lt;span class="math"&gt;\(\sqrt d_k\)&lt;/span&gt; 进行缩放；&lt;/li&gt;
&lt;li&gt;然后通过 softmax 得到每个 value 的权重；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实际在 GPU 上跑时，多个 quries 打包成一个矩阵 &lt;span class="math"&gt;\(Q\)&lt;/span&gt; 后并行计算，同理 key 和 value 也打包成矩阵 &lt;span class="math"&gt;\(K\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(V\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt d_k}) V\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;attention 的矩阵形式&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第零步，输入序列被转化成 3 类数据 &lt;span class="math"&gt;\(Q\)&lt;/span&gt;，&lt;span class="math"&gt;\(K\)&lt;/span&gt;，&lt;span class="math"&gt;\(V\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;输入序列一共有 n 个 token，每个 token 被 embedding 映射成 1 个维度为 &lt;span class="math"&gt;\(d_{model}\)&lt;/span&gt; 的向量表示，则所有 token 的向量表示组合在一起形成一个矩阵 &lt;span class="math"&gt;\(X \in \mathbb{R}^{n \times d_{model}}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;矩阵 &lt;span class="math"&gt;\(X\)&lt;/span&gt; 分别和矩阵 &lt;span class="math"&gt;\(W^Q \in \mathbb{R}^{d_{model} \times d_{model}}\)&lt;/span&gt;，&lt;span class="math"&gt;\(W^K \in \mathbb{R}^{d_{model} \times d_{model}}\)&lt;/span&gt;，&lt;span class="math"&gt;\(W^V \in \mathbb{R}^{d_{model} \times d_{model}}\)&lt;/span&gt; 矩阵乘得到&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(Q \in \mathbb{R}^{n \times d_{model}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(K \in \mathbb{R}^{n \times d_{model}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(V \in \mathbb{R}^{n \times d_{model}}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="QKV" src="/images/attention-is-all-you-need-note/QKV.png"&gt;&lt;/p&gt;
&lt;p&gt;第一步，打分。计算 query 和 keys 之间的相似度，可以通过 dot-product 完成。相似度越高，说明这个 key 和 query 越相关。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(QK^T \in \mathbb{R}^{n \times n}\)&lt;/span&gt;，每一行的每个 element 表示该行 token 与所有 token 之间的相关性。&lt;/p&gt;
&lt;p&gt;&lt;img alt="QKT" src="/images/attention-is-all-you-need-note/QKT.png"&gt;&lt;/p&gt;
&lt;p&gt;第二步，计算权重。将这些相似度通过 softmax 函数转化为概率权重。&lt;/p&gt;
&lt;p&gt;sofmax 后矩阵维度不变，仍为 &lt;span class="math"&gt;\(n \times n\)&lt;/span&gt;，每 1 行对应一个 token 与其他 token 的相关性，每个行向量求和是 1。&lt;/p&gt;
&lt;p&gt;&lt;img alt="softmax" src="/images/attention-is-all-you-need-note/softmax.png"&gt;&lt;/p&gt;
&lt;p&gt;第三步，加权求和。用这些权重对所有 value 进行加权求和，相关性强的 value 向量权重大，相关性弱的 value 向量权重小。这样模型就输出了一个综合所有相关值的信息，而且突出了重要信息而忽略了不相关信息。&lt;/p&gt;
&lt;p&gt;用每个行向量和 V 矩阵的对应行相乘，行向量的 element 广播乘 V 矩阵的行向量，如 element0 和 V 矩阵第一个行向量做广播乘，element1 和 V 矩阵第二个行向量做广播乘，依次类推。这些加权后的 value 向量之间做 elementwise 的求和，结果是一个行向量，维度为 &lt;span class="math"&gt;\(1 \times d_{model}\)&lt;/span&gt;，为输入 token 的编码输出结果。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Z1" src="/images/attention-is-all-you-need-note/Z1.png"&gt;&lt;/p&gt;
&lt;p&gt;因为一共有 n 个 token，所以输出矩阵的维度为 &lt;span class="math"&gt;\(n \times d_{model}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Z" src="/images/attention-is-all-you-need-note/Z.png"&gt;&lt;/p&gt;
&lt;p&gt;可以看出来，每个 token 的输出向量不再是独立的，而是包含了上下文信息。&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;最常见的 attention 有两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;additive attention：用一个单层的前馈网络计算兼容性；&lt;/li&gt;
&lt;li&gt;dot-product attention：和本文的 scaled dot-prodcut attention 相似，唯一的区别是不包含除以缩放因子 &lt;span class="math"&gt;\(\sqrt d_k\)&lt;/span&gt; 的步骤；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;尽管两种 attention 的计算量相同，但是 dot-product attention 要快得多，空间效率也更高，因为它可以利用高度优化的矩阵乘代码加速。&lt;/p&gt;
&lt;p&gt;Q：为什么需要缩放因子？&lt;/p&gt;
&lt;p&gt;A：当 &lt;span class="math"&gt;\(d_k\)&lt;/span&gt; 很大时，点积的幅值会很大，导致 softmax 的梯度非常小，所以需要除以 &lt;span class="math"&gt;\(\sqrt d_k\)&lt;/span&gt;。&lt;/p&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;比如 q 和 k 都是均值为 0，方差为 1 的向量，则它们的点积的均值为 0，方差为 &lt;span class="math"&gt;\(d_k\)&lt;/span&gt;，为了抵消这种影响，需要将点积缩放 &lt;span class="math"&gt;\(\frac{1}{\sqrt d_k}\)&lt;/span&gt; 倍。&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id="multi-head-attention"&gt;Multi-Head Attention&lt;/h4&gt;
&lt;p&gt;相比于维度为 &lt;span class="math"&gt;\(d_{model}\)&lt;/span&gt; 的单头 attention，multi-head 能更有效地捕捉信息，因为 multi-head 能同时关注到不同位置的多种特征信息，相比之下只有一个 head 则平均化了这些信息，从而限制了模型的表达能力。&lt;/p&gt;
&lt;p&gt;multi-head attention 算法过程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把 queries，keys，values 分别 linear projection 映射 h 次，每次用不同的学习到的参数，且每次映射后的 queries，keys，values 的维度缩小为 &lt;span class="math"&gt;\(d_k\)&lt;/span&gt;，&lt;span class="math"&gt;\(d_k\)&lt;/span&gt;，&lt;span class="math"&gt;\(d_v\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;在每次映射后的 queries，keys，values 版本上的计算 attention，每次产生 &lt;span class="math"&gt;\(d_v\)&lt;/span&gt; 维的 output；&lt;/li&gt;
&lt;li&gt;将这些 output concat 到一起后再做一次 linear projection，得到最终 output；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class="math"&gt;\(MultiHead(Q, K, V) = Concat(head_1, head_2, \dots, head_h)W^O\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W^Q \in \mathbb{R}^{d_{model} \times d_k}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W^K \in \mathbb{R}^{d_{model} \times d_k}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W^V \in \mathbb{R}^{d_{model} \times d_v}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W^O \in \mathbb{R}^{hd_v \times d_{model}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在本文中 &lt;span class="math"&gt;\(h = 8\)&lt;/span&gt;，&lt;span class="math"&gt;\(d_k = d_v = d_{model}/h = 64\)&lt;/span&gt;，因为每个 head 的维度都变小了，所以总计算量和单一 attention 近似相同。&lt;/p&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;从 &lt;span class="math"&gt;\(W^Q\)&lt;/span&gt;，&lt;span class="math"&gt;\(W^K\)&lt;/span&gt;，&lt;span class="math"&gt;\(W^V\)&lt;/span&gt;，&lt;span class="math"&gt;\(W^O\)&lt;/span&gt; 的维度可以看出来，multi-head 就是将单 head 的大维度 &lt;span class="math"&gt;\(d_{model}\)&lt;/span&gt; 等分成了 h 份，分别算出各自的 attention output 后重新 concat 到一起恢复出原来的大维度。&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img alt="decoder" src="/images/attention-is-all-you-need-note/transformer_multi-headed_self-attention-recap.png"&gt;&lt;/p&gt;
&lt;h4 id="multi-head-attention-in-model"&gt;Multi-head Attention in Model&lt;/h4&gt;
&lt;p&gt;transformer 以 3 种方式应用 multi-head attention：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;encoder-decoder attention：queries 来自前一个 decoder layer，而 keys 和 values 来自 encoder output，这样使得 decoder 的每个位置都能关注到所有输入位置。&lt;/li&gt;
&lt;li&gt;encoder 中的 self-attention：所有的 queries，keys 和 values 都来自同一个地方 —— 前一个 encoder layer 的输出。encoder layer 中的每个位置都能关注到前一个 attention layer 的所有位置。&lt;/li&gt;
&lt;li&gt;decoder 中的 self-attention：所有位置都能关注到当前位置及以前位置，为了保持 decoder 的自回归性，需要阻止 decoder 中的信息向左流动。具体是通过 mask 将 softmax 的非法输入的值改为 &lt;span class="math"&gt;\(-\infty\)&lt;/span&gt; 来实现。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;一般的 encoder-decoder 架构中 source 和 target 不同，attention 发生在 target 的 query 和 source 的所有 key，value 之间，而 self-attention 发生在 source 内部，或者 target 内部。&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;单个 output token 的产生过程：&lt;/p&gt;
&lt;p&gt;&lt;img alt="decoder-1" src="/images/attention-is-all-you-need-note/transformer_decoding_1.gif"&gt;&lt;/p&gt;
&lt;p&gt;多个 output token 自回归过程：&lt;/p&gt;
&lt;p&gt;&lt;img alt="decoder-2" src="/images/attention-is-all-you-need-note/transformer_decoding_2.gif"&gt;&lt;/p&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;encoder stack 的最终输出是 K，V 矩阵，用于每个 decoder 的 encoder-decoder attention。所有 encoder 只在训练时有用，推理时只需要 decoder 就足够了。&lt;/p&gt;
&lt;p&gt;Q：decoder attention 仍然是 attention，每次输出一个 token 对应的一个维度为 &lt;span class="math"&gt;\(1 \times d_{model}\)&lt;/span&gt; 的向量，如何从这个向量得到输出 token？&lt;/p&gt;
&lt;p&gt;A：linear + softmax。&lt;/p&gt;
&lt;p&gt;首先 linear，即 fc 层，将 decoder 的输出 vector 映射为一个非常非常大的 logits vector，该 vector 的维度即 model 的字典大小，每个值表示对应字的 score。&lt;/p&gt;
&lt;p&gt;其次，softmax 将每个 score 转化为概率，然后选择概率最大的字输出。&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="point-wise-feed-forward-networks"&gt;Point-wise Feed-Forward Networks&lt;/h3&gt;
&lt;p&gt;每个 encoder layer 和 decoder layer 中除了 attention sub-layer 之外，还包含一个 FFN。该网络结构为两个线性变换中间插入了一个 ReLU。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;虽然所有位置都使用相同的线性变换公式，但是每层不同位置的 weight 都不同，所以这个线性变换也可以当成是 k=1 的 convolution。&lt;/p&gt;
&lt;p&gt;输入、输出的 &lt;span class="math"&gt;\(d_{model} = 512\)&lt;/span&gt;，中间层的维度为 &lt;span class="math"&gt;\(d_{ff} = 2048\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id="embedding-and-softmax"&gt;Embedding and Softmax&lt;/h3&gt;
&lt;p&gt;和其他序列转换模型类似，本文也通过 learned embedding 将输入、输出 token 映射成维度为 &lt;span class="math"&gt;\(d_{model}\)&lt;/span&gt; 的 vector。&lt;/p&gt;
&lt;p&gt;本文也使用常见的基于 learned linear transformation 和 softmax 将 decoder 的输出转化为下一个 token 出现的概率。&lt;/p&gt;
&lt;p&gt;在我们的模型中，两个 embedding layer 和 pre-softmax linear transformation 共享相同的 weight，其中 embedding layers 给 weight 乘以 &lt;span class="math"&gt;\(\sqrt{d_{model}}\)&lt;/span&gt; 进行了缩放。&lt;/p&gt;
&lt;h3 id="positional-embedding"&gt;Positional Embedding&lt;/h3&gt;
&lt;p&gt;因为网络中没有 RNN 和 CNN，所以为了建模序列的顺序信息，必须注入序列中 token 的相对和绝对位置信息，具体方法是：&lt;/p&gt;
&lt;p&gt;将 positional embedding 添加到 encoder 和 decoder 的 embedding layer 中，而且 positional embedding 的维度也为 &lt;span class="math"&gt;\(d_{model}\)&lt;/span&gt;，以便两者可以求和。&lt;/p&gt;
&lt;p&gt;positional embedding 的方法有很多种（学习到的 or 固定参数的），本文使用的方法是不同频率的 sin 和 cos 函数：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(PE_{(pos, 2i)} = sin(pos/1000^{2i/d_{model}})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(PE_{(pos, 2i+1)} = cos(pos/1000^{2i/d_{model}})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中 &lt;span class="math"&gt;\(pos\)&lt;/span&gt; 为 position，&lt;span class="math"&gt;\(i\)&lt;/span&gt; 为 dimension。即 position embedding 的每个维度都是一个 sin 曲线。&lt;/p&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;选择 sin 曲线的好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sin 函数是一个周期函数，当 sequence 变得比训练 sequence 更长时也能算出来；&lt;/li&gt;
&lt;li&gt;可以很容易地根据相对位置算出 positional embedding：&lt;span class="math"&gt;\(sin(A+B) = sin(A)cos(B) + cos(A)sin(B)\)&lt;/span&gt;；&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;positional embedding 的值是直接 element-wise 加到 embedding 的值上面的，而不是 concat。&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;试验表明 learned 方式效果相同，但是我们选择了 sin 函数的版本，因为它使模型更能适应比训练更长的序列。&lt;/p&gt;
&lt;h2 id="why-self-attention"&gt;Why Self-Attention&lt;/h2&gt;
&lt;p&gt;本文对比了 self-attention layer 和其他序列转换 encoder-decoder 中常见的 RNN/CNN layer，基于以下 3 点考虑最终选择了 self-attention。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每层的计算量；&lt;/li&gt;
&lt;li&gt;可并行计算的比例，通过所需的最小顺序计算量来衡量；&lt;/li&gt;
&lt;li&gt;网络中 long-range dependencies 之间的 path-length（序列转换中的关键挑战，path-length 越短模型学习 long-range dependencies 的能力越强）；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="complex" src="/images/attention-is-all-you-need-note/complex.png"&gt;&lt;/p&gt;
&lt;p&gt;对于第二点顺序计算量：&lt;/p&gt;
&lt;p&gt;self-attention 连接了任意两个位置，所以任意两个位置之间相关性的顺序计算量为常数 O(1) ，即只需要一步就可以得到，而 RNN 需要迭代 O(n) 次顺序操作才能得到；&lt;/p&gt;
&lt;p&gt;对于第一点计算量：&lt;/p&gt;
&lt;p&gt;当序列长度 n &amp;lt; 表示维度 d 时，self-attention 比 RNN 更快。一般来说都满足这个条件。对于很长的序列，则 self-attention 不如 RNN，但是这个问题可以通过限制 self-attention 的半径来解决。&lt;/p&gt;
&lt;p&gt;kernel 大小 k &amp;lt; n 的 conv 无法连接所有的 input-ouput pair，如果要实现这个目标，需要级联 O(n/k) 个 conv 或者 &lt;span class="math"&gt;\(O(log_k(n))\)&lt;/span&gt; 个 dilated conv。一般来说 conv 的复杂度约为 RNN 的 k 倍。separable conv 可以降低复杂度到 &lt;span class="math"&gt;\(O(k \cdot n \cdot d + n \cdot k^2)\)&lt;/span&gt;，当 n = k 时，它的复杂度和本文所用的 self-attention + point-wise forward layer 相同。&lt;/p&gt;
&lt;p&gt;self-attention 的另外一个好处是模型更加可解释，不仅每个 head 学习到不同的任务，多个 head 还表现出和句子语法和语义相关的行为。&lt;/p&gt;
&lt;h2 id="result"&gt;Result&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;单一 head 比 multi-head 差；&lt;/li&gt;
&lt;li&gt;head 数量过多也会变差；&lt;/li&gt;
&lt;li&gt;减小 &lt;span class="math"&gt;\(d_k\)&lt;/span&gt; 会损害效果，设计比点积更复杂的 competibility function 可能是有益的；&lt;/li&gt;
&lt;li&gt;更大的模型一般效果更好；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;本文提出了 transformer 网络：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一个完全基于 attention 的序列转换模型，用 multi-head self-attention 代替传统 encoder-decoder 中的 recurrent layer；&lt;/li&gt;
&lt;li&gt;对于翻译任务，transformer 的训练速度明显比其他基于 RNN 和 CNN 的模型更快，翻译效果 SOTA；&lt;/li&gt;
&lt;li&gt;计划将 transformer 扩展到除 text 之外其他模态的应用中；&lt;/li&gt;
&lt;li&gt;计划研究 local，restricted transformer 来处理大数据量的 input/output，比如 image，audio，video；&lt;/li&gt;
&lt;li&gt;另外一个研究目标：让生成过程尽量避免顺序执行；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="ref"&gt;Ref&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://easyai.tech/ai-definition/encoder-decoder-seq2seq/"&gt;Encoder-Decoder 和 Seq2Seq&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://jalammar.github.io/illustrated-transformer/"&gt;The Illustrated Transformer&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'blue ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category><category term="Method"></category><category term="Transformer"></category><category term="Attention"></category></entry><entry><title>我的 PKM</title><link href="https://qian-gu.github.io/posts/misc/my-pkm.html" rel="alternate"></link><published>2014-05-06T15:57:00+08:00</published><updated>2014-05-06T15:57:00+08:00</updated><author><name>Qian Gu</name></author><id>tag:qian-gu.github.io,2014-05-06:/posts/misc/my-pkm.html</id><summary type="html">&lt;p&gt;总结学习习惯和 PKM 工具。&lt;/p&gt;</summary><content type="html">&lt;h2 id="pkm"&gt;PKM&lt;/h2&gt;
&lt;h3 id="pkm_1"&gt;什么是 PKM&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;个人知识管理 (Personal Knowledge Management) 是一种新的知识管理的理念和方法，能将个人拥有的各种资料、随手可得的信息变成更具价值的知识，最终利于自己的工作、生活。通过对个人知识的管理，人们可以养成良好的学习习惯 , 增强信息素养，完善自己的专业知识体系 , 提高自己的能力和竞争力，为实现个人价值和可持续发展打下坚实基础。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;知乎上的&lt;a href="http://www.zhihu.com/question/19576595"&gt;回答&lt;/a&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;个人知识管理的概念最早是由 Paul Dorsey 教授提出的 , 原文为 ” Personal Knowledge Management should be viewed as a set a problem-solving skills that have both a logical or conceptual as well as physical or hands-on component. ”, 中文意思就是 : 个人知识管理应该被看作既有逻辑概念层面又有实际操作层面的一套解决问题的技巧与方法。所以个人知识管理主要讲的是一种方法论。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Personal-knowledge-management"&gt;Personal knowledge management on wikipedia&lt;/a&gt; &lt;/p&gt;
&lt;h3 id="blog"&gt;为什么写这篇 blog&lt;/h3&gt;
&lt;p&gt;从小到大，上了这么多年的学，每个人都有自己的学习习惯和知识管理方法。这篇 blog 是在我 Google、知乎问答、参考相关博客内容的基础上，总结出的学习习惯和使用各种工具的经验，写这篇博客的目的不是秀逼格有多高（估计很多人都看不上），也不是所谓的教程（根本不够资格），而是&lt;strong&gt;为了分享和讨论&lt;/strong&gt; :-D&lt;/p&gt;
&lt;h2 id="_1"&gt;获取知识&lt;/h2&gt;
&lt;p&gt;总结我的知识来源，大致可以分为 3 类：纸质书籍、网络资源、其他 。分别总结如下&lt;/p&gt;
&lt;h3 id="_2"&gt;书籍&lt;/h3&gt;
&lt;p&gt;书籍上的知识占到了我总来源的 50% 。相比于电子读物，我更喜欢阅读纸质书籍。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;纸质书籍更适合深度阅读&lt;/p&gt;
&lt;p&gt;这就是为什么很多码农桌子上都有那么几本经典著作，虽然他们的电脑里面也保存着相应的电子版。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;纸和笔是最原始的阅读工具，但是也是最有效，或者 &lt;em&gt;最自由的&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;这是目前电子书籍 / 笔记最大的缺点，写一段标注文字，画一个草图，列一个表格，插入一个公式，不仅需要花费很多时间，而且操作很繁琐，结果也往往差强人意 。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;阅读体验&lt;/p&gt;
&lt;p&gt;毫无疑问纸质书籍的阅读体验更好一些，不然 Kindle 也就不会出现了 :-D&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;至于读书笔记，我的方法很原始 =.=&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;书边笔记，这样笔记和书是一体的，如果空白不够大，有便利贴&lt;/li&gt;
&lt;li&gt;标记符号，简单的标记，比如 &lt;code&gt;？&lt;/code&gt; 表示疑问、&lt;code&gt;！&lt;/code&gt; 表示注意、&lt;code&gt;||&lt;/code&gt; 表示分层、&lt;code&gt;*&lt;/code&gt; 表示重要内容、圈出关键名词 / 定义 etc&lt;/li&gt;
&lt;li&gt;整理笔记，定期将书本上的笔记整理到 Blog / Evernote 中&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_3"&gt;网络&lt;/h3&gt;
&lt;p&gt;互联网时代，善于利用互联网上的资源，相当于守着一座 “ 知识金山 ”&lt;/p&gt;
&lt;p&gt;每天面对海量的信息，如果不加以过滤，我们就会被淹没在信息的海洋之中，浪费了大量时间而毫无收益。我的信息来源：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;RSS 订阅&lt;/p&gt;
&lt;p&gt;以前使用 Google Reader，自从 GR 死了之后，转到了 &lt;a href="http://cloud.feedly.com"&gt;Feedly&lt;/a&gt; 。主要是搜集一些大牛的博客，订阅 &lt;em&gt;真正&lt;/em&gt; 感兴趣的内容。&lt;/p&gt;
&lt;p&gt;RSS 订阅是个时间黑洞，曾经我的 RSS 订阅长期 1000+，而且我从来没有看完过，作为处女座强迫症患者，有时候花费了很多时间，甚至占用上课学习时间。后来看到 &lt;a href="http://mindhacks.cn/"&gt;刘未鹏&lt;/a&gt; 在 &lt;a href="http://book.douban.com/subject/6709809/"&gt;《暗时间》&lt;/a&gt; 中有一篇文章讲自己的学习习惯，才明白原来这是源自人不肯 “ 关上一扇门 ” 的心理。&lt;strong&gt;事实上，真正宝贵的信息，在其他来源你也可以接触到。&lt;/strong&gt;于是，我退订了很多从来不看或者很少看的源，从此，告别阅读焦虑。:-D&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Google / Wiki&lt;/p&gt;
&lt;p&gt;拒绝做伸手党，我们遇到的很多问题实际上别人都遇到过了，并且提供了解决方法。在网上发帖问别人的时候，很多问题都可以从 FAQ、官方文档中都能找到答案。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;知乎 / Quora / Stack Overflow&lt;/p&gt;
&lt;p&gt;这绝对是一个值得一生相伴的好东西 :-P 长期混迹于此，瞬间高大上的境界、令人仰视的逼格不再是梦想，相信我，你的人生因此而改变！（利益相关：非广告，深度沉迷者）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;论坛 / 豆瓣小组&lt;/p&gt;
&lt;p&gt;当然是讨论知识的论坛，而非八卦娱乐主题的论坛。长期泡论坛，不仅可以寻找答案，帮助别人，还可以扩展自己的见识，结识大牛，论坛里面的老油条个个都是大神。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用到的工具：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://cloud.feedly.com"&gt;feedly&lt;/a&gt; 是一款 RSS 阅读器&lt;/p&gt;
&lt;p&gt;同类产品还有很多，比如国外的 Diggo、国内的鲜果、豆瓣九点等 。feedly 在国内访问并不是很稳定，即使这样，我仍然选择它是因为舒服的排版、另外功能上有些源 Diggo 无法抓取到 。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://getpocket.com/a/"&gt;pocket&lt;/a&gt; 是一个用于管理互联网文章的应用程序&lt;/p&gt;
&lt;p&gt;PC 上有 chrome 插件，手机上也有相应的客户端，这样我们就可以很方便地在各个平台上、把各个源的文章都收集到一起，稍后阅读（比如排队、休息时）。&lt;/p&gt;
&lt;p&gt;我还它当作汇聚资料的容器使用，比如我写这篇博客，从网上搜集到很多相关内容，把我认为重要的保存在 pocket 当中，然后再做整理 。&lt;/p&gt;
&lt;p&gt;P.S. 还有另外一个方法，feedly 上的文章可以直接保存到 Evernote 中，但是这是付费功能，虽然可以用 ifttt 解决，但是这种方法无法收集其他地方的文章，而且，不知道是不是因为天朝的原因，ifttt 收集的速度并不快，一般要 30 分钟左右才能在 Evernote 中看到收集的文章，最重要的一点，在 Evernote 中看文章，眼睛一直盯着屏幕的右下角，都快成斜眼了 T-T，so 还是 pocket 好！&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_4"&gt;其他&lt;/h3&gt;
&lt;p&gt;所谓其他，就是在其它地方，看到的、想到的，使用手机肯定是最方便的。我只使用过两款应用 Google Keep 和 Evernote 。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://zh.wikipedia.org/wiki/Google-Keep"&gt;Google Keep&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Google 发布的笔记软件，和其他各种笔记类软件相比，Keep 的功能实在是 “ 弱爆了 ”，或许称之为 &lt;em&gt;便签 sticker&lt;/em&gt; 更合适 .&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://evernote.com"&gt;Evernote&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;大名鼎鼎的笔记软件，功能非常强大。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有很多人把这两个软件做比较&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.zhihu.com/question/20869752"&gt;Google Keep VS Evernote，你还会继续使用 Evernote 吗？为什么？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.zhihu.com/question/20859926"&gt;刚宣布 Reader 关停，Google Keep 随即上线，这是用来跟 Evernote 竞争的产品吗？谷歌做这个有什么优势？谷歌如何让用户有安全感地放心使用旗下产品？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;事实上，这两个产品的定位根本就不一样，引用里面的某个答案&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Google Keep 偏重于随时随地记录碎片知识 , 建立待办事项 , 是个效率工具 . 完成的事情就随手一滑存档掉 , 并没有提供更一步的存储整理的途径 . 所以 &lt;strong&gt;Google Keep 并不是知识的终点&lt;/strong&gt;, 它只是用来记录临时想法的 , 这恰恰和它的 logo 形象十分契合 ( 想想卡通里面人物突然奇思妙想了 ... 灯泡就亮了 - -). 它就是用来存储这些 " 灯泡 " 的 .&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一般，我使用 Keep 来记录平时的想法，也把它当作便签来使用，记录杂事，有部分 to-do-list 的作用。毕竟，Evernote 缓慢的启动速度和不方便的操作实在是太影响心情了 。&lt;/p&gt;
&lt;p&gt;还有一个答案，不能同意更多：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;归根到底，&lt;strong&gt;keep 是一个轻量级 GTD 系统，它要解决的问题和 evernote 是不同的。那些认为 keep 可以取代 evernote 的用户，他们本身就不需要 evernote&lt;/strong&gt; —— 对他们而言，evernote 的强大反而太过繁冗，已是多余。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="_5"&gt;整理知识&lt;/h2&gt;
&lt;p&gt;获取到知识，下一步就是整理和吸收它们了。&lt;/p&gt;
&lt;h3 id="evernote"&gt;Evernote&lt;/h3&gt;
&lt;p&gt;说道 PKM，就不得不提 Evernote 。这么一款大名鼎鼎的笔记软件，有很多人在使用，分享使用技巧 。搜集到一个很全的资源帖：&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.douban.com/group/topic/28544265/"&gt;印象笔记 Evernote 教程 (douban)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;里面总结了官方的教程、民间高手的使用心得。基本上花费一下午的时间在豆瓣 &amp;amp; 知乎上逛一逛，就可以成为高手了 。&lt;/p&gt;
&lt;p&gt;我不是 EN 的高级用户，也没有专门去探索进阶用法，我只是把它当作单纯的电子笔记本，不代替便签（Google Keep），也不代替知识管理软件（Wiz、OneNote），一般也就以下几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保存博客草稿，毕竟 Keep 这样的便签不适合于写大段文字&lt;/li&gt;
&lt;li&gt;记录私人笔记&lt;/li&gt;
&lt;li&gt;摘抄网络上的精彩内容（也是为写 blog 服务），EN 在 chrome 上的 web clipper 插件真心不错&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;忍不住跑题说一下我的使用感受：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;：全平台、全功能、云同步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;：大而无当，这是一款变形金刚一样的软件，功能强大到有人为此写出一本厚厚的&lt;a href="http://book.douban.com/subject/24524405/"&gt;使用教程&lt;/a&gt; 。但是，强大的功能也就意味着软件的笨重和操作的繁琐。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;和便签类应用相比&lt;/p&gt;
&lt;p&gt;它太笨重了！ 与 Keep 简洁的操作（快速新建、左右滑动归档、撤销），这货简直慢的让人无法忍受。知乎上有个贴子专门讨论这个问题：&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.zhihu.com/question/20820355"&gt;为什么有人对印象笔记没有任何兴趣呢？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其中有个回答，也是我的感受：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;你明知道它的功能正是你需要的，但打開它就是不想用，也不知道從何用起。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;和知识管理类应用相比&lt;/p&gt;
&lt;p&gt;仅个人意见，EN 并不适合作知识管理 。它只有 笔记本 和 笔记本组 这两个概念，也就是说，它最多支持 2 级目录，而实际上，我们的知识体系应该是和树一样，不断地分支增长的 。其他笔记类应用比如 Wiz 笔记就要比它好很多 。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; 我曾经也为用 Keep 还是 EN 纠结过，最终我选择了 Keep，直到后来我看到这句话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;那些认为 keep 可以取代 evernote 的用户，他们本身就不需要 evernote&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我才明白，我根本就不需要 EN！我曾经尝试过在 EN 中建立一个完备正交的目录，后来发现，其中最重要的部分和我的博客目录是一模一样的 。我已经有个博客了，为什么还要 EN 呢？（个人认为 blog 比 EN 更好，原因后文解释）&lt;/p&gt;
&lt;p&gt;事实上很多人根本就没有管理知识的必要，因为他们没有那么多知识需要整理。使用 Evernote 的动机无非就是人类的天性 ——“ 收藏 ” 癖好，看到什么东西都想收藏，比如用过的袋子，小本子，现在轮到网页，图片，链接了。可是我们收藏了以后还会去看么？就像我们硬盘里面收集的 “ 世界一百部名著 ” 一样，它静静地躺在硬盘中，积累了厚厚的灰 。&lt;/p&gt;
&lt;p&gt;EN 的 logo 是一只大象，寓意来自美国谚语 “An elephant never forgets”，大象的记忆力很强，这和它的宣传 “Evernote 是你的第二个大脑 ” 是一致的，而且是一个永远不会忘记的大脑。&lt;/p&gt;
&lt;p&gt;所以，为什么不就这么用呢？在 EN 中新建几个笔记本，把我们不需要记住但是以后可能会用到的 “ 知识 ” 丢进去，加上简单的标签，然后我们只需要记住在笔记里有这么个记录，然后其他的全都可以忘掉了，需要的时候去搜索一下就可以了。利用它强大的接口功能，记录杂事、美食、私人日记、生活小感悟、小常识，但是对于体系结构化的知识，个人认为写一个技术博客是更优的选择（理由在后文） :-P&lt;/p&gt;
&lt;p&gt;漫长的跑题终于结束了，总之就是，&lt;strong&gt;把 EN 当作收集小知识的笔记本即可，个人知识管理选择其他软件 / 写博客是更优的选择。给它做做减法，使用你需要的功能，不要成为工具的奴隶。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="xmind"&gt;XMind&lt;/h3&gt;
&lt;p&gt;目前还处于用笔和纸的阶段，大概了解了一些 XMind 的用途，感觉很好很强大。尤其是用它建立的结构化的提纲，对于整理文章、知识体系结构有巨大的帮助。还在探索中，更加进阶的功能和使用心得以后再总结。&lt;/p&gt;
&lt;h2 id="_6"&gt;使用 / 分享知识&lt;/h2&gt;
&lt;p&gt;月光博客中的一段话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;知识共享和传播，是为了让别人知道你知道的知识，并得到信息的反馈，为什么要分享，不在于你认识什么人，而在于什么人认识你，不在于什么人影响了你，而在于你影响了什么人。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;获取、整理知识，接下来就是使用 / 分享知识。只有把别人的东西转化为自己的，才算是掌握了。我觉得最好的方法就是写 Blog。&lt;/p&gt;
&lt;h3 id="blog_1"&gt;为什么要写 Blog&lt;/h3&gt;
&lt;p&gt;这个问题曾经我也问过自己，以前只是看到学长学姐们写的 Blog，感觉很厉害，认为自己也应该有这样一个。后来看到 &lt;a href="http://book.douban.com/subject/6709809/"&gt;暗时间&lt;/a&gt; 里面的两篇文章：&lt;/p&gt;
&lt;p&gt;&lt;a href="http://mindhacks.cn/2009/02/09/writing-is-better-thinking/"&gt;《书写是为了更好的思考》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://mindhacks.cn/2009/02/15/why-you-should-start-blogging-now/"&gt;《为什么你从现在开始就应该写博客》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;认真总结了一下写博客的好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;总结知识&lt;/p&gt;
&lt;p&gt;这是我写 Blog 最直接的一个动机。只有把别人的东西转化为自己的，才算是真正掌握了。记录下自己某段时间的学习收获，思考感悟，不仅可以帮助自己回忆以前的知识，说不定还可以帮助到别人（我就是受益者，从别人的博客中学习到了很多知识）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;帮助思考&lt;/p&gt;
&lt;p&gt;这是写 Blog 最大的好处。这个好处不仅仅局限于写 Blog 这种形式，无论以什么方式，只要写下你的思考过程和内容，就可以帮助你思考问题，也就是 &lt;strong&gt;书写&lt;/strong&gt; 的好处。&lt;a href="http://book.douban.com/subject/6709809/"&gt;《暗时间》&lt;/a&gt; 里面有个很形象的比喻：&lt;em&gt;人的思考就像是在黑暗中打着手电筒前行。&lt;/em&gt;因为人的脑力资源是有限的，所以我们经常遇到想问题想岔了，然后回不到原点的情况。书写不仅可以帮助我们缓存手电筒照到的区域，还可以帮助我们扩大手电筒的照亮直径。比如我写这篇博客，开始只是记录下别人的 PKM 的方法和工具的关键字，然后在大纲和简记的基础上，补充自己的心得完成的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;得到交流帮助&lt;/p&gt;
&lt;p&gt;Blog 相比于 EN 的最大的优势。写 Blog 是开放的，别人可以随意看，而写 EN 我们只能自己看（EN 的共享笔记体验并不好）。每个人的思考都有盲点，你的盲点可以在别人那里得到补充，知识和思考在讨论得到提炼升华。&lt;/p&gt;
&lt;p&gt;还有一个好处是，敦促你必须认真思考。想象你在给一个玩偶小熊讲一个原理、技术，那么你必须 &lt;em&gt;“ 彻底反思整个知识体系，弄清这座大厦的根基在什么地方，弄清它的骨架在什么地方，一砖一瓦到底是怎么垒起来的。设想自己在 11 层，给处于 1 层的小熊讲明白一件事。”&lt;/em&gt; 现在你不需要小熊了，因为有更挑剔的的对手 —— 你的博客的读者。你偷懒不写 / 写出无意义的文章，他们不会像小熊一样包容你，只有写出高质量的博客才对得起观众，否则就是浪费他们的时间了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;锻炼心智&lt;/p&gt;
&lt;p&gt;激励你去坚持学习和思考：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;为了让你的博客有价值，你必须不断地总结自己学习的结果，比必须不断思考，给出比别人深刻、读到的见解。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;让你学会持之以恒地做一件事：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;理性地书写的时候，大脑逐渐进入推理分析模块，一切不愉快的情绪，烦躁感都会逐渐消隐下去。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上述体验在写代码时也能体会到。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;锻炼文字功底&lt;/p&gt;
&lt;p&gt;对于码农，这算是优点么？反正没有坏处 :-P&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="_7"&gt;写在最后&lt;/h2&gt;
&lt;p&gt;总结一下总的流程就是：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;获取知识&lt;/strong&gt;：书籍 + feedly + pocket + Google Keep&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;整理知识&lt;/strong&gt;：Evernote + XMind&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;使用 / 分享知识&lt;/strong&gt;：Blog&lt;/p&gt;
&lt;p&gt;总结出来并且在实际行动中加以利用才是王道。&lt;/p&gt;
&lt;h2 id="_8"&gt;参考&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.sand-ox.com/2011/02/pkm-tools/"&gt;我，和我的知识工具们 (2011-02-15 更新 )&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.williamlong.info/archives/3388.html"&gt;我的知识管理工具列表&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.zhihu.com/topic/19557303"&gt;Evernote on zhihu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.zhihu.com/topic/19558457"&gt;PKM on zhihu&lt;/a&gt;&lt;/p&gt;</content><category term="Misc"></category><category term="PKM"></category></entry></feed>