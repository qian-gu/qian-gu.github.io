
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="index, follow" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://qian-gu.github.io/theme/stylesheet/style.min.css">

    <link id="dark-theme-style" rel="stylesheet" type="text/css"
          media="(prefers-color-scheme: dark)"
    href="https://qian-gu.github.io/theme/stylesheet/dark-theme.min.css">

    <link id="pygments-dark-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: dark)"
          href="https://qian-gu.github.io/theme/pygments/monokai.min.css">
    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"
          href="https://qian-gu.github.io/theme/pygments/monokai.min.css">


  <link rel="stylesheet"
        type="text/css"
        href="https://qian-gu.github.io/theme/stork/stork.css" />

  <link rel="stylesheet" type="text/css" href="https://qian-gu.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://qian-gu.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://qian-gu.github.io/theme/font-awesome/css/solid.css">

  <link rel="stylesheet" type="text/css" href="/static/custom.css">

  <link rel="shortcut icon" href="https://qian-gu.github.io/images/favicon_64x64.ico" type="image/x-icon">
  <link rel="icon" href="https://qian-gu.github.io/images/favicon_64x64.ico" type="image/x-icon">

  <!-- Chrome, Firefox OS and Opera -->
  <meta name="theme-color" content="#333333">
  <!-- Windows Phone -->
  <meta name="msapplication-navbutton-color" content="#333333">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <!-- Microsoft EDGE -->
  <meta name="msapplication-TileColor" content="#333333">

  <link href="https://qian-gu.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Qian's Blog Atom">


<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-48826831-1', 'auto');
  ga('send', 'pageview');
</script>






 

<meta name="author" content="Qian Gu" />
<meta name="description" content="Transformer 论文笔记。" />
<meta name="keywords" content="Method, Transformer, Attention">


  <meta property="og:site_name" content="Qian's Blog"/>
  <meta property="og:title" content="《Attention Is All You Need》 笔记"/>
  <meta property="og:description" content="Transformer 论文笔记。"/>
  <meta property="og:locale" content="en"/>
  <meta property="og:url" content="https://qian-gu.github.io/posts/misc/attention-is-all-you-need-bi-ji.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2025-10-11 23:30:00+08:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="https://qian-gu.github.io/author/qian-gu.html">
  <meta property="article:section" content="misc"/>
  <meta property="article:tag" content="Method"/>
  <meta property="article:tag" content="Transformer"/>
  <meta property="article:tag" content="Attention"/>
  <meta property="og:image" content="https://qian-gu.github.io/images/logo.png">

  <title>Qian's Blog &ndash; 《Attention Is All You Need》 笔记</title>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-1821536199377100",
      enable_page_level_ads: true
    });
  </script>

</head>
<body >

<aside>
  <div>
    <a href="https://qian-gu.github.io/">
      <img src="https://qian-gu.github.io/images/logo.png" alt="Qian Gu" title="Qian Gu">
    </a>

    <h1>
      <a href="https://qian-gu.github.io/">Qian Gu</a>
    </h1>

    <p>Read >> Think >> Write</p>

    <div class="stork">
      <input class="stork-input" type="text" autocomplete="off" name="q" data-stork="sitesearch" placeholder="Search..." onclick="loadStorkIndex()"/>
      <div class="stork-output" data-stork="sitesearch-output"></div>
    </div>

    <nav>
      <ul class="list">


            <li>
              <a target="_blank"
                 href="https://qian-gu.github.io/pages/about-me.html#about-me">
                About Me
              </a>
            </li>

      </ul>
    </nav>

    <ul class="social">
      <li>
        <a class="sc-envelope"
rel="me"           href="mailto:guqian110@163.com"
           target="_blank">
          <i class="fa-solid fa-envelope"></i>
        </a>
      </li>
      <li>
        <a class="sc-github"
           href="https://github.com/qian-gu"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
      <li>
        <a class="sc-twitter"
           href="https://twitter.com/qian_gu"
           target="_blank">
          <i class="fa-brands fa-twitter"></i>
        </a>
      </li>
      <li>
        <a class="sc-rss"
           href="/feeds/all.atom.xml"
           target="_blank">
          <i class="fa-solid fa-rss"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>

<nav>
  <a href="https://qian-gu.github.io/">Home</a>

  <a href="/authors.html">Authors</a>
  <a href="/archives.html">Archives</a>
  <a href="/categories.html">Categories</a>
  <a href="/tags.html">Tags</a>

  <a href="https://qian-gu.github.io/feeds/all.atom.xml">Atom</a>

</nav>

<article class="single">
  <header>
    <link href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.1.0/css/all.css" rel="stylesheet">
      
    <h1 id="attention-is-all-you-need-bi-ji">《Attention Is All You Need》 笔记</h1>
    <p>
      Posted on 2025-10-11 23:30 in <a href="https://qian-gu.github.io/category/misc.html">misc</a>

    </p>
    <div class="tag-cloud">
      <p>
        <a href="https://qian-gu.github.io/tag/method.html">Method</a>
        <a href="https://qian-gu.github.io/tag/transformer.html">Transformer</a>
        <a href="https://qian-gu.github.io/tag/attention.html">Attention</a>
      </p>
    </div>
  </header>



  <div>
    <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#backgroud">Backgroud</a></li>
<li><a href="#model-architecture">Model Architecture</a><ul>
<li><a href="#encoder-and-decoder-stack">encoder and decoder stack</a></li>
<li><a href="#attention">Attention</a><ul>
<li><a href="#scaled-dot-product-attention">Scaled Dot-Product Attention</a></li>
<li><a href="#multi-head-attention">Multi-Head Attention</a></li>
<li><a href="#multi-head-attention-in-model">Multi-head Attention in Model</a></li>
</ul>
</li>
<li><a href="#point-wise-feed-forward-networks">Point-wise Feed-Forward Networks</a></li>
<li><a href="#embedding-and-softmax">Embedding and Softmax</a></li>
<li><a href="#positional-embedding">Positional Embedding</a></li>
</ul>
</li>
<li><a href="#why-self-attention">Why Self-Attention</a></li>
<li><a href="#result">Result</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#ref">Ref</a></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>背景知识：encoder-decoder</strong></p>
<p>encoder-decoder 模型是 NLP 领域的概念，不是指具体的算法，而是一类算法的统称，是一个通用框架，在这个框架下可以使用不同算法解决不同任务。</p>
<p>encoder 的作用：将现实问题转化为数学问题。</p>
<p><img alt="encoder" src="/images/attention-is-all-you-need-note/encoder.png.webp"></p>
<p>decoder 的作用：将数学问题转化为现实问题。</p>
<p><img alt="decoder" src="/images/attention-is-all-you-need-note/decoder.png.webp"></p>
<p>两个连在一起：</p>
<p><img alt="encoder-decoder" src="/images/attention-is-all-you-need-note/encoder-decoder.png.webp"></p>
<p>需要注意的两点：</p>
<ol>
<li>无论输入输出的长度是多少，中间向量 C 的长度固定，显然长序列会有数据损失。</li>
<li>根据任务不同，encoder 和 decoder 可以用不同的视线，如 RNN，LSTM 或 GRU 等。</li>
</ol>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>背景知识：seq2seq</strong></p>
<p>seq2seq 如字面意思，输入一个 sequence，输出一个 sequence，重点在于 sequence 的长度是可变的。</p>
<p><img alt="nmt-model-fast" src="/images/attention-is-all-you-need-note/nmt-model-fast.gif"></p>
<p>典型的序列转换模型通常包括一个编码器（encoder）和一个解码器（decoder）。encoder 负责将输入序列编码成一个固定长度的隐状态表示，而 decoder 则利用这个隐状态表示生成目标序列。</p>
<p>在这些模型中，循环神经网络（RNN）和卷积神经网络（CNN）是从前最常见的架构。然而，2017 年以来，基于注意力机制的 Transformer 架构（即本论文介绍的架构）因其并行计算能力和处理长距离依赖关系的优势，成为序列转换任务中的新宠。</p>
<p>在 transformer 之前，主流方法是 RNN 或者 CNN，但是两者都有各自的缺点：</p>
<ul>
<li>RNN 通过隐藏状态可以记住所有历史，但是隐藏状态只能递归计算，无法并行化；</li>
<li>CNN 可以并行计算，但是对长距离 token 之间相关性的建模能力很弱。它只能对 kernel size 内的 token 之间的相关性建模，如果需要建模长距离相关性，则必须级联很多层；</li>
</ul>
<p>transformer 试图结合两者的优点：既能建模长距离，又能并行化。</p>
<p>Q：seq2seq 和 encoder-decoder 的区别：</p>
<p>A：seq2seq 强调目的，encoder-decoder 强调方法，seq2seq 使用的方法基本上都属于 encoder-decoder 模型。</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>背景知识：embedding</strong></p>
<p>计算机要处理任何信息都必须先将其转化成数值，比如人类可以理解的单词，但是因为每个单词有多重属性，只转化出 1 个值能表达信息的能力有限，所以一般会转成多个值，这些值组合在一起形成 1 个向量，即该 token 的向量表示 <code>embedding vector</code>。</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>背景知识：attention</strong></p>
<p>encoder-docoder 架构中的中间向量 C 是固定长度，所以对于长序列，压缩后的会有信息丢失，算法效果不好。attention 机制中的 encoder 编码完的结果不再是一个固定长度的中间向量 C，而是一个向量序列，这样就能解决这个问题。</p>
<p><img alt="attention" src="/images/attention-is-all-you-need-note/attention-arch.png.webp"></p>
<p>主要有两个特点：</p>
<ul>
<li>没有信息丢失。</li>
<li>重要 token 和次要 token 权重不同。</li>
</ul>
<p>attention 机制和人类处理信息的方式类似：优先关注重要信息。我们阅读一段文字时，不会平均分配注意力到每个字上面，而是会重点阅读和问题（query）相关的文字（key 和 value）。</p>
<p>attention 通过计算 query 和 key 的相似度，动态地调整对 input 的关注程度，所以能更有效地处理复杂任务。在数学上 attention 机制就是对 Source 中的元素的 Value 加权求和，而 Query 和 Key 用来计算对应 Value 的权重系数：</p>
<p><span class="math">\(Attention(Query, Source) = \sum_{i=1}^{L_x}a_i * Value_i\)</span></p>
<p>其中 <span class="math">\(L_x\)</span> 表示序列的长度。</p>
<p><span class="math">\(a_i = sofmax(Sim_i) = \frac{e^{Sim_i}}{\sum_{j=1}^{L_x}e^{Sim_j}}\)</span></p>
<p>因为两个 token 对应两个向量，所以衡量两个 token 之间的相似度 <span class="math">\(Sim_i\)</span> 也就变成了衡量两个向量之间的相似度。一般使用的方法有：</p>
<ul>
<li>cos 相似度：<span class="math">\(s(q, k) = \frac{q^T k}{\lvert q \rvert \cdot \lvert k \rvert}\)</span></li>
<li>向量点积：<span class="math">\(s(q, k) = q^T k\)</span></li>
</ul>
<p>attention 一般分为下图的 3 个步骤：</p>
<ul>
<li>第一步，query 和 key 计算相似度，得到打分 score；</li>
<li>第二步，将 score 归一化，得到每个 value 的 weight；</li>
<li>第三步，用 weight 对 value 加权求和；</li>
</ul>
<p><img alt="attention" src="/images/attention-is-all-you-need-note/attention.png"></p>
<p>注意：上图中的 query，key，value 可以是标量，也可以是向量。</p>
</div>
<h2 id="introduction">Introduction</h2>
<ul>
<li>RNN、LSTM、特别是 Gated Recurrent Neural Network 在语言建模和机器翻译等序列建模和转换任务中已经是公认的 SOTA 方法。业界有很多方法持续扩展递归语言模型和 encoder-decoder 架构的能力边界。</li>
<li>循环模型一般沿着输入和输出进行计算，根据前一隐藏状态 <span class="math">\(h_{t-1}\)</span> 和当前位置 <span class="math">\(t\)</span> 计算当前隐藏状态 <span class="math">\(h_t\)</span>，<strong>这种递归顺序阻碍了训练的并行化。特别是在长序列中更加明显，因为有限的内存阻碍了跨样本的 batch 并行处理。</strong></li>
<li>尽管通过 factorization trick 和 conditional computation 可以大幅提高递归模型的计算效率，在某种情况下还提高了模型性能，但是递归的顺序计算约束仍然存在。</li>
<li>在各种任务中，attention 机制已经成为序列模型和转换任务重不可或缺的组成部分，因为它可以在不考虑 sequence 中 token 的距离的情况下建立依赖关系。但是除了少数情况，大部分情况下 attention 都和循环网络结合使用。</li>
<li>本文提出一种名为 <code>transformer</code> 的新模型架构，完全抛弃了递归网络，只依赖 attention 机制捕捉输入输出之间的全局相关性。这种架构可以有更高的并行度，只需要在 8 个 P100 上训练 12 个小时就能在翻译任务上达到 SOTA。</li>
</ul>
<h2 id="backgroud">Backgroud</h2>
<ul>
<li>减少顺序计算的需求也催生了 ByteNet 和 CONVS2S 等模型，它们都采用 CNN 作为基本 block，并行计算所有输入、输出之间的 hidden representation。</li>
<li>但是这些模型中，在任意两个输入、输出位置之间建立相关性需要的操作数，会随着位置距离的增加而增加，在 ByteNet 中线性关系，在 CONVS2S 中为对数关系。所以学习远距离之间的依赖变得很困难。</li>
<li>transformer 可以将这个复杂度降低为常数，但是代价是平均注意力 weight 可能会降低有效分辨率，我们通过 multihead attention 机制来克服这个问题。</li>
<li>self-attention 是一种通过序列不同位置之间的相关性来计算序列 representation 的 attention 机制，应用在很多任务中。</li>
<li>transformer 是第一个只依赖 attention 机制的模型。</li>
</ul>
<h2 id="model-architecture">Model Architecture</h2>
<p>大部分序列转换 model 都基于 encoder-decoder 架构：</p>
<ul>
<li>encoder 把输入序列 <span class="math">\((x_1, x_2, \dots, x_n)\)</span> 转化为一个连续的向量表示 <span class="math">\(z = (z_1, z_2, \dots, z_n)\)</span>；</li>
<li>给定 <span class="math">\(z\)</span>，decoder 以一次生成 1 个字符的方式生成输出序列 <span class="math">\((y_1, y_2, \dots, y_n)\)</span>，每一步都是自回归的，即每次都会将之前的输出也作为输入的一部分；</li>
</ul>
<p>transformer 也遵循这样的结构，encoder 和 decoder 都基于堆叠的 self-attention 和 point-wise fc 层。</p>
<p><img alt="model-arch" src="/images/attention-is-all-you-need-note/model-arch.png"></p>
<h3 id="encoder-and-decoder-stack">encoder and decoder stack</h3>
<p>encoder 由 N = 6 个完全相同的 layer 堆叠组成，每个 layer 由 2 个 sub-layer 组成：</p>
<ul>
<li>第一个 sub-layer 是 multi-head self-attention；</li>
<li>第二个 sub-layer 是一个简单的 point-wise fc 前馈网络；</li>
</ul>
<p>每个 sub-layer 的输出都采用 residual 连接后接一个 layer norm 层，最终输出为 <span class="math">\(LayerNorm(x + sublayer(x))\)</span>，其中 <span class="math">\(sublayer(x)\)</span> 为 sub-layer 本身的函数功能。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul>
<li>residual 的目的是防止网络退化；</li>
<li>layer norm 的目的是对每一层的 activation 进行归一化；</li>
</ul>
</div>
<p>为了实现 residual 连接，所有 sub-layer，包括 embedding 在内，输出维度均为 <span class="math">\(d_{model} = 512\)</span>，一般设置为训练时的最长 sequence 的 token 数量。</p>
<p>decoder 也由 N = 6 个完全相同的 layer 堆叠组成，除了 encoder 中的两个 sub-layer 外，decoder 中还额外插入了第三个 sub-layer，该 sub-layer 对 encoder output 做 multi-head attention 处理。 </p>
<p>和 encoder 类似，decoder 的每个 sub-layer 也使用 residual + layer norm 连接。此外 decoder 中的 self-attention 还做了特殊设计，以防止后续位置的信息被添加到当前位置信息中，这种 mask 机制和 embedding 的偏移机制相结合，可以确保位置 i 只依赖小于 i 的已知输出，保证了 decoder 的自回归性。</p>
<h3 id="attention">Attention</h3>
<p>attention 机制是一种将 query 和 1 组 key-value pair 映射为 output 的过程，其中 query，keys, values，output 都是 vector。</p>
<p>output 是 values 的加权求和，每个 value 的权重由 query 和对应的 key 的 competibility function 计算得到。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>本文用到 competibility function 是 dot-product。</p>
</div>
<h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4>
<p><img alt="attention" src="/images/attention-is-all-you-need-note/multi-head-attention.png"></p>
<p>本文的 attention 叫做 scaled dot-product attention：</p>
<ul>
<li>输入为 queries 和 keys，维度均为 <span class="math">\(d_k\)</span>；</li>
<li>计算 queries 和所有 keys 之间的点乘计算相关性，然后除以 <span class="math">\(\sqrt d_k\)</span> 进行缩放；</li>
<li>然后通过 softmax 得到每个 value 的权重；</li>
</ul>
<p>实际在 GPU 上跑时，多个 quries 打包成一个矩阵 <span class="math">\(Q\)</span> 后并行计算，同理 key 和 value 也打包成矩阵 <span class="math">\(K\)</span> 和 <span class="math">\(V\)</span>。</p>
<p><span class="math">\(Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt d_k}) V\)</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>attention 的矩阵形式</strong></p>
<p>第零步，输入序列被转化成 3 类数据 <span class="math">\(Q\)</span>，<span class="math">\(K\)</span>，<span class="math">\(V\)</span>：</p>
<p>输入序列一共有 n 个 token，每个 token 被 embedding 映射成 1 个维度为 <span class="math">\(d_{model}\)</span> 的向量表示，则所有 token 的向量表示组合在一起形成一个矩阵 <span class="math">\(X \in \mathbb{R}^{n \times d_{model}}\)</span>。</p>
<p>矩阵 <span class="math">\(X\)</span> 分别和矩阵 <span class="math">\(W^Q \in \mathbb{R}^{d_{model} \times d_{model}}\)</span>，<span class="math">\(W^K \in \mathbb{R}^{d_{model} \times d_{model}}\)</span>，<span class="math">\(W^V \in \mathbb{R}^{d_{model} \times d_{model}}\)</span> 矩阵乘得到</p>
<ul>
<li><span class="math">\(Q \in \mathbb{R}^{n \times d_{model}}\)</span></li>
<li><span class="math">\(K \in \mathbb{R}^{n \times d_{model}}\)</span></li>
<li><span class="math">\(V \in \mathbb{R}^{n \times d_{model}}\)</span></li>
</ul>
<p><img alt="QKV" src="/images/attention-is-all-you-need-note/QKV.png"></p>
<p>第一步，打分。计算 query 和 keys 之间的相似度，可以通过 dot-product 完成。相似度越高，说明这个 key 和 query 越相关。</p>
<p><span class="math">\(QK^T \in \mathbb{R}^{n \times n}\)</span>，每一行的每个 element 表示该行 token 与所有 token 之间的相关性。</p>
<p><img alt="QKT" src="/images/attention-is-all-you-need-note/QKT.png"></p>
<p>第二步，计算权重。将这些相似度通过 softmax 函数转化为概率权重。</p>
<p>sofmax 后矩阵维度不变，仍为 <span class="math">\(n \times n\)</span>，每 1 行对应一个 token 与其他 token 的相关性，每个行向量求和是 1。</p>
<p><img alt="softmax" src="/images/attention-is-all-you-need-note/softmax.png"></p>
<p>第三步，加权求和。用这些权重对所有 value 进行加权求和，相关性强的 value 向量权重大，相关性弱的 value 向量权重小。这样模型就输出了一个综合所有相关值的信息，而且突出了重要信息而忽略了不相关信息。</p>
<p>用每个行向量和 V 矩阵的对应行相乘，行向量的 element 广播乘 V 矩阵的行向量，如 element0 和 V 矩阵第一个行向量做广播乘，element1 和 V 矩阵第二个行向量做广播乘，依次类推。这些加权后的 value 向量之间做 elementwise 的求和，结果是一个行向量，维度为 <span class="math">\(1 \times d_{model}\)</span>，为输入 token 的编码输出结果。</p>
<p><img alt="Z1" src="/images/attention-is-all-you-need-note/Z1.png"></p>
<p>因为一共有 n 个 token，所以输出矩阵的维度为 <span class="math">\(n \times d_{model}\)</span>。</p>
<p><img alt="Z" src="/images/attention-is-all-you-need-note/Z.png"></p>
<p>可以看出来，每个 token 的输出向量不再是独立的，而是包含了上下文信息。</p>
</div>
<p>最常见的 attention 有两类：</p>
<ul>
<li>additive attention：用一个单层的前馈网络计算兼容性；</li>
<li>dot-product attention：和本文的 scaled dot-prodcut attention 相似，唯一的区别是不包含除以缩放因子 <span class="math">\(\sqrt d_k\)</span> 的步骤；</li>
</ul>
<p>尽管两种 attention 的计算量相同，但是 dot-product attention 要快得多，空间效率也更高，因为它可以利用高度优化的矩阵乘代码加速。</p>
<p>Q：为什么需要缩放因子？</p>
<p>A：当 <span class="math">\(d_k\)</span> 很大时，点积的幅值会很大，导致 softmax 的梯度非常小，所以需要除以 <span class="math">\(\sqrt d_k\)</span>。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>比如 q 和 k 都是均值为 0，方差为 1 的向量，则它们的点积的均值为 0，方差为 <span class="math">\(d_k\)</span>，为了抵消这种影响，需要将点积缩放 <span class="math">\(\frac{1}{\sqrt d_k}\)</span> 倍。</p>
</div>
<h4 id="multi-head-attention">Multi-Head Attention</h4>
<p>相比于维度为 <span class="math">\(d_{model}\)</span> 的单头 attention，multi-head 能更有效地捕捉信息，因为 multi-head 能同时关注到不同位置的多种特征信息，相比之下只有一个 head 则平均化了这些信息，从而限制了模型的表达能力。</p>
<p>multi-head attention 算法过程：</p>
<ul>
<li>把 queries，keys，values 分别 linear projection 映射 h 次，每次用不同的学习到的参数，且每次映射后的 queries，keys，values 的维度缩小为 <span class="math">\(d_k\)</span>，<span class="math">\(d_k\)</span>，<span class="math">\(d_v\)</span>。</li>
<li>在每次映射后的 queries，keys，values 版本上的计算 attention，每次产生 <span class="math">\(d_v\)</span> 维的 output；</li>
<li>将这些 output concat 到一起后再做一次 linear projection，得到最终 output；</li>
</ul>
<p><span class="math">\(MultiHead(Q, K, V) = Concat(head_1, head_2, \dots, head_h)W^O\)</span></p>
<p><span class="math">\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\)</span></p>
<p>其中，</p>
<p><span class="math">\(W^Q \in \mathbb{R}^{d_{model} \times d_k}\)</span></p>
<p><span class="math">\(W^K \in \mathbb{R}^{d_{model} \times d_k}\)</span></p>
<p><span class="math">\(W^V \in \mathbb{R}^{d_{model} \times d_v}\)</span></p>
<p><span class="math">\(W^O \in \mathbb{R}^{hd_v \times d_{model}}\)</span></p>
<p>在本文中 <span class="math">\(h = 8\)</span>，<span class="math">\(d_k = d_v = d_{model}/h = 64\)</span>，因为每个 head 的维度都变小了，所以总计算量和单一 attention 近似相同。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>从 <span class="math">\(W^Q\)</span>，<span class="math">\(W^K\)</span>，<span class="math">\(W^V\)</span>，<span class="math">\(W^O\)</span> 的维度可以看出来，multi-head 就是将单 head 的大维度 <span class="math">\(d_{model}\)</span> 等分成了 h 份，分别算出各自的 attention output 后重新 concat 到一起恢复出原来的大维度。</p>
</div>
<p><img alt="decoder" src="/images/attention-is-all-you-need-note/transformer_multi-headed_self-attention-recap.png"></p>
<h4 id="multi-head-attention-in-model">Multi-head Attention in Model</h4>
<p>transformer 以 3 种方式应用 multi-head attention：</p>
<ul>
<li>encoder-decoder attention：queries 来自前一个 decoder layer，而 keys 和 values 来自 encoder output，这样使得 decoder 的每个位置都能关注到所有输入位置。</li>
<li>encoder 中的 self-attention：所有的 queries，keys 和 values 都来自同一个地方 —— 前一个 encoder layer 的输出。encoder layer 中的每个位置都能关注到前一个 attention layer 的所有位置。</li>
<li>decoder 中的 self-attention：所有位置都能关注到当前位置及以前位置，为了保持 decoder 的自回归性，需要阻止 decoder 中的信息向左流动。具体是通过 mask 将 softmax 的非法输入的值改为 <span class="math">\(-\infty\)</span> 来实现。</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>一般的 encoder-decoder 架构中 source 和 target 不同，attention 发生在 target 的 query 和 source 的所有 key，value 之间，而 self-attention 发生在 source 内部，或者 target 内部。</p>
</div>
<p>单个 output token 的产生过程：</p>
<p><img alt="decoder-1" src="/images/attention-is-all-you-need-note/transformer_decoding_1.gif"></p>
<p>多个 output token 自回归过程：</p>
<p><img alt="decoder-2" src="/images/attention-is-all-you-need-note/transformer_decoding_2.gif"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>encoder stack 的最终输出是 K，V 矩阵，用于每个 decoder 的 encoder-decoder attention。所有 encoder 只在训练时有用，推理时只需要 decoder 就足够了。</p>
<p>Q：decoder attention 仍然是 attention，每次输出一个 token 对应的一个维度为 <span class="math">\(1 \times d_{model}\)</span> 的向量，如何从这个向量得到输出 token？</p>
<p>A：linear + softmax。</p>
<p>首先 linear，即 fc 层，将 decoder 的输出 vector 映射为一个非常非常大的 logits vector，该 vector 的维度即 model 的字典大小，每个值表示对应字的 score。</p>
<p>其次，softmax 将每个 score 转化为概率，然后选择概率最大的字输出。</p>
</div>
<h3 id="point-wise-feed-forward-networks">Point-wise Feed-Forward Networks</h3>
<p>每个 encoder layer 和 decoder layer 中除了 attention sub-layer 之外，还包含一个 FFN。该网络结构为两个线性变换中间插入了一个 ReLU。</p>
<p><span class="math">\(FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\)</span></p>
<p>虽然所有位置都使用相同的线性变换公式，但是每层不同位置的 weight 都不同，所以这个线性变换也可以当成是 k=1 的 convolution。</p>
<p>输入、输出的 <span class="math">\(d_{model} = 512\)</span>，中间层的维度为 <span class="math">\(d_{ff} = 2048\)</span>。</p>
<h3 id="embedding-and-softmax">Embedding and Softmax</h3>
<p>和其他序列转换模型类似，本文也通过 learned embedding 将输入、输出 token 映射成维度为 <span class="math">\(d_{model}\)</span> 的 vector。</p>
<p>本文也使用常见的基于 learned linear transformation 和 softmax 将 decoder 的输出转化为下一个 token 出现的概率。</p>
<p>在我们的模型中，两个 embedding layer 和 pre-softmax linear transformation 共享相同的 weight，其中 embedding layers 给 weight 乘以 <span class="math">\(\sqrt{d_{model}}\)</span> 进行了缩放。</p>
<h3 id="positional-embedding">Positional Embedding</h3>
<p>因为网络中没有 RNN 和 CNN，所以为了建模序列的顺序信息，必须注入序列中 token 的相对和绝对位置信息，具体方法是：</p>
<p>将 positional embedding 添加到 encoder 和 decoder 的 embedding layer 中，而且 positional embedding 的维度也为 <span class="math">\(d_{model}\)</span>，以便两者可以求和。</p>
<p>positional embedding 的方法有很多种（学习到的 or 固定参数的），本文使用的方法是不同频率的 sin 和 cos 函数：</p>
<p><span class="math">\(PE_{(pos, 2i)} = sin(pos/1000^{2i/d_{model}})\)</span></p>
<p><span class="math">\(PE_{(pos, 2i+1)} = cos(pos/1000^{2i/d_{model}})\)</span></p>
<p>其中 <span class="math">\(pos\)</span> 为 position，<span class="math">\(i\)</span> 为 dimension。即 position embedding 的每个维度都是一个 sin 曲线。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>选择 sin 曲线的好处：</p>
<ul>
<li>sin 函数是一个周期函数，当 sequence 变得比训练 sequence 更长时也能算出来；</li>
<li>可以很容易地根据相对位置算出 positional embedding：<span class="math">\(sin(A+B) = sin(A)cos(B) + cos(A)sin(B)\)</span>；</li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>positional embedding 的值是直接 element-wise 加到 embedding 的值上面的，而不是 concat。</p>
</div>
<p>试验表明 learned 方式效果相同，但是我们选择了 sin 函数的版本，因为它使模型更能适应比训练更长的序列。</p>
<h2 id="why-self-attention">Why Self-Attention</h2>
<p>本文对比了 self-attention layer 和其他序列转换 encoder-decoder 中常见的 RNN/CNN layer，基于以下 3 点考虑最终选择了 self-attention。</p>
<ol>
<li>每层的计算量；</li>
<li>可并行计算的比例，通过所需的最小顺序计算量来衡量；</li>
<li>网络中 long-range dependencies 之间的 path-length（序列转换中的关键挑战，path-length 越短模型学习 long-range dependencies 的能力越强）；</li>
</ol>
<p><img alt="complex" src="/images/attention-is-all-you-need-note/complex.png"></p>
<p>对于第二点顺序计算量：</p>
<p>self-attention 连接了任意两个位置，所以任意两个位置之间相关性的顺序计算量为常数 O(1) ，即只需要一步就可以得到，而 RNN 需要迭代 O(n) 次顺序操作才能得到；</p>
<p>对于第一点计算量：</p>
<p>当序列长度 n &lt; 表示维度 d 时，self-attention 比 RNN 更快。一般来说都满足这个条件。对于很长的序列，则 self-attention 不如 RNN，但是这个问题可以通过限制 self-attention 的半径来解决。</p>
<p>kernel 大小 k &lt; n 的 conv 无法连接所有的 input-ouput pair，如果要实现这个目标，需要级联 O(n/k) 个 conv 或者 <span class="math">\(O(log_k(n))\)</span> 个 dilated conv。一般来说 conv 的复杂度约为 RNN 的 k 倍。separable conv 可以降低复杂度到 <span class="math">\(O(k \cdot n \cdot d + n \cdot k^2)\)</span>，当 n = k 时，它的复杂度和本文所用的 self-attention + point-wise forward layer 相同。</p>
<p>self-attention 的另外一个好处是模型更加可解释，不仅每个 head 学习到不同的任务，多个 head 还表现出和句子语法和语义相关的行为。</p>
<h2 id="result">Result</h2>
<ul>
<li>单一 head 比 multi-head 差；</li>
<li>head 数量过多也会变差；</li>
<li>减小 <span class="math">\(d_k\)</span> 会损害效果，设计比点积更复杂的 competibility function 可能是有益的；</li>
<li>更大的模型一般效果更好；</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>本文提出了 transformer 网络：</p>
<ul>
<li>第一个完全基于 attention 的序列转换模型，用 multi-head self-attention 代替传统 encoder-decoder 中的 recurrent layer；</li>
<li>对于翻译任务，transformer 的训练速度明显比其他基于 RNN 和 CNN 的模型更快，翻译效果 SOTA；</li>
<li>计划将 transformer 扩展到除 text 之外其他模态的应用中；</li>
<li>计划研究 local，restricted transformer 来处理大数据量的 input/output，比如 image，audio，video；</li>
<li>另外一个研究目标：让生成过程尽量避免顺序执行；</li>
</ul>
<h2 id="ref">Ref</h2>
<p><a href="https://easyai.tech/ai-definition/encoder-decoder-seq2seq/">Encoder-Decoder 和 Seq2Seq</a></p>
<p><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'blue ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>


  <div class="neighbors">
    <a class="btn float-left" href="https://qian-gu.github.io/posts/tools/verible.html" title="开源 IC 工具/库 —— verible">
      <i class="fa fa-angle-left"></i> Previous Post
    </a>
  </div>


    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <ins class="adsbygoogle ads-responsive"
         data-ad-client="ca-pub-1821536199377100"
         data-ad-slot="4843941849"></ins>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
    </script>

<!-- Gitalk -->
<div id="gitalk-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="https://qian-gu.github.io/theme/gitalk/md5.min.js"></script>
<script>
var gitalk = new Gitalk({
  clientID: '4b3de26a6e80be727416',
  clientSecret: '993d79339c842fc56d9739ef268f38806dc93f50',
  repo: 'qian-gu.github.io',
  owner: 'qian-gu',
  admin: ['qian-gu'],
  id: md5(location.href),
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- End Gitalk -->
</article>

<footer>
<p>
  &copy; 2025  - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
  <span class="footer-separator">|</span>
  Switch to the <a href="javascript:void(0)" onclick="theme.switch(`dark`)">dark</a> | <a href="javascript:void(0)" onclick="theme.switch(`light`)">light</a> | <a href="javascript:void(0)" onclick="theme.switch(`browser`)">browser</a> theme
  <script id="dark-theme-script"
          src="https://qian-gu.github.io/theme/dark-theme/dark-theme.min.js"
          data-enable-auto-detect-theme="True"
          data-default-theme="ligtht"
          type="text/javascript">
  </script>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
           src="https://qian-gu.github.io/theme/img/cc/by-sa.png"
         width="80"
         height="15"/>
  </a>
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Qian's Blog ",
  "url" : "https://qian-gu.github.io",
  "image": "https://qian-gu.github.io/images/logo.png",
  "description": "Qian's Thoughts and Writings"
}
</script><a href="https://github.com/qian-gu/qian-gu.github.io" target="_blank" class="github-corner" aria-label="View source on Github">
    <svg width="80"
         height="80"
         viewBox="0 0 250 250"
         style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;"
         aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor"
              style="transform-origin: 130px 106px;"
              class="octo-arm">
        </path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor"
              class="octo-body">
        </path>
    </svg>
</a>
  <script>
    window.loadStorkIndex = function () {
      stork.initialize("https://qian-gu.github.io/theme/stork/stork.wasm")
      stork.register("sitesearch", "https://qian-gu.github.io/search-index.st", { showProgress: false });
    }
  </script>
  <script src="https://qian-gu.github.io/theme/stork/stork.js"></script>

</body>
</html>