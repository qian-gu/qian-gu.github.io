
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="index, follow" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://qian-gu.github.io/theme/stylesheet/style.min.css">

    <link id="dark-theme-style" rel="stylesheet" type="text/css"
          media="(prefers-color-scheme: dark)"
    href="https://qian-gu.github.io/theme/stylesheet/dark-theme.min.css">

    <link id="pygments-dark-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: dark)"
          href="https://qian-gu.github.io/theme/pygments/monokai.min.css">
    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"
          href="https://qian-gu.github.io/theme/pygments/monokai.min.css">


  <link rel="stylesheet"
        type="text/css"
        href="https://qian-gu.github.io/theme/stork/stork.css" />

  <link rel="stylesheet" type="text/css" href="https://qian-gu.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://qian-gu.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://qian-gu.github.io/theme/font-awesome/css/solid.css">

  <link rel="stylesheet" type="text/css" href="/static/custom.css">

  <link rel="shortcut icon" href="https://qian-gu.github.io/images/favicon_64x64.ico" type="image/x-icon">
  <link rel="icon" href="https://qian-gu.github.io/images/favicon_64x64.ico" type="image/x-icon">

  <!-- Chrome, Firefox OS and Opera -->
  <meta name="theme-color" content="#333333">
  <!-- Windows Phone -->
  <meta name="msapplication-navbutton-color" content="#333333">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <!-- Microsoft EDGE -->
  <meta name="msapplication-TileColor" content="#333333">

  <link href="https://qian-gu.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Qian's Blog Atom">


<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-48826831-1', 'auto');
  ga('send', 'pageview');
</script>






 

<meta name="author" content="Qian Gu" />
<meta name="description" content="总结 Cache 的设计细节" />
<meta name="keywords" content="Computer Architecture, Cache">


  <meta property="og:site_name" content="Qian's Blog"/>
  <meta property="og:title" content="Computer Architecture 笔记 —— Cache"/>
  <meta property="og:description" content="总结 Cache 的设计细节"/>
  <meta property="og:locale" content="en"/>
  <meta property="og:url" content="https://qian-gu.github.io/posts/ic/ca-cache.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2021-11-20 19:21:00+08:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="https://qian-gu.github.io/author/qian-gu.html">
  <meta property="article:section" content="IC"/>
  <meta property="article:tag" content="Computer Architecture"/>
  <meta property="article:tag" content="Cache"/>
  <meta property="og:image" content="https://qian-gu.github.io/images/logo.png">

  <title>Qian's Blog &ndash; Computer Architecture 笔记 —— Cache</title>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-1821536199377100",
      enable_page_level_ads: true
    });
  </script>

</head>
<body >

<aside>
  <div>
    <a href="https://qian-gu.github.io/">
      <img src="https://qian-gu.github.io/images/logo.png" alt="Qian Gu" title="Qian Gu">
    </a>

    <h1>
      <a href="https://qian-gu.github.io/">Qian Gu</a>
    </h1>

    <p>Read >> Think >> Write</p>

    <div class="stork">
      <input class="stork-input" type="text" autocomplete="off" name="q" data-stork="sitesearch" placeholder="Search..." onclick="loadStorkIndex()"/>
      <div class="stork-output" data-stork="sitesearch-output"></div>
    </div>

    <nav>
      <ul class="list">


            <li>
              <a target="_blank"
                 href="https://qian-gu.github.io/pages/about-me.html#about-me">
                About Me
              </a>
            </li>

      </ul>
    </nav>

    <ul class="social">
      <li>
        <a class="sc-envelope"
rel="me"           href="mailto:guqian110@163.com"
           target="_blank">
          <i class="fa-solid fa-envelope"></i>
        </a>
      </li>
      <li>
        <a class="sc-github"
           href="https://github.com/qian-gu"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
      <li>
        <a class="sc-twitter"
           href="https://twitter.com/qian_gu"
           target="_blank">
          <i class="fa-brands fa-twitter"></i>
        </a>
      </li>
      <li>
        <a class="sc-rss"
           href="/feeds/all.atom.xml"
           target="_blank">
          <i class="fa-solid fa-rss"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>

<nav>
  <a href="https://qian-gu.github.io/">Home</a>

  <a href="/authors.html">Authors</a>
  <a href="/archives.html">Archives</a>
  <a href="/categories.html">Categories</a>
  <a href="/tags.html">Tags</a>

  <a href="https://qian-gu.github.io/feeds/all.atom.xml">Atom</a>

</nav>

<article class="single">
  <header>
    <link href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.1.0/css/all.css" rel="stylesheet">
      
    <h1 id="ca-cache">Computer Architecture 笔记 —— Cache</h1>
    <p>
      Posted on 2021-11-20 19:21 in <a href="https://qian-gu.github.io/category/ic.html">IC</a>

    </p>
    <div class="tag-cloud">
      <p>
        <a href="https://qian-gu.github.io/tag/computer-architecture.html">Computer Architecture</a>
        <a href="https://qian-gu.github.io/tag/cache.html">Cache</a>
      </p>
    </div>
  </header>


  <div class="related-posts">
    <h4>Part 1 of the Computer Architecture 笔记 series</h4>
  </div>

  <div>
    <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#cache">为什么需要 Cache？</a><ul>
<li><a href="#_1">面临的问题</a></li>
<li><a href="#_2">解决方案</a></li>
</ul>
</li>
<li><a href="#cache_1">如何确定 Cache 规格？</a><ul>
<li><a href="#_3">容量选择</a></li>
<li><a href="#block">Block 大小选择</a></li>
<li><a href="#_4">映射方式</a></li>
<li><a href="#_5">替换策略</a></li>
<li><a href="#_6">写回策略</a><ul>
<li><a href="#write-throughwt-write-allocate">Write-Through(WT) + write allocate</a></li>
<li><a href="#write-backwb-write-allocate">Write-Back(WB) + write allocate</a></li>
<li><a href="#write-aroundwa">Write-Around(WA)</a></li>
<li><a href="#write-invalidwi">Write-Invalid(WI)</a></li>
<li><a href="#write-onlywo">Write-Only(WO)</a></li>
<li><a href="#pass-throughpt">Pass-Through(PT)</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
</li>
<li><a href="#arm-m-cache">实例：Arm M 系列 cache 规格</a></li>
<li><a href="#arm-a-cache">实例：Arm A 系列 cache 规格</a></li>
<li><a href="#t-head-c906">实例：T-head C906</a></li>
</ul>
</li>
<li><a href="#cache_2">Cache 的性能指标</a><ul>
<li><a href="#amat">AMAT</a></li>
<li><a href="#_7">其他指标</a></li>
</ul>
</li>
<li><a href="#cache_3">Cache 的性能分析模型</a></li>
<li><a href="#cache_4">Cache 性能优化方法</a><ul>
<li><a href="#small-and-simple-l1-cache">Small and Simple L1 Cache</a></li>
<li><a href="#way-prediction">Way Prediction</a></li>
<li><a href="#vitural-indexphysical-tag">Vitural Index/Physical Tag</a></li>
<li><a href="#serial">Serial 访问</a></li>
<li><a href="#cache-size">Cache Size</a></li>
<li><a href="#block-size">Block Size</a></li>
<li><a href="#set-associativity">Set Associativity</a></li>
<li><a href="#prefetch">Prefetch</a></li>
<li><a href="#software-optimize">Software Optimize</a></li>
<li><a href="#vicitm-cache">Vicitm Cache</a></li>
<li><a href="#write-buffer">Write Buffer( 读优先 )</a></li>
<li><a href="#write-merging">Write Merging</a></li>
<li><a href="#multiple-level">Multiple Level</a></li>
<li><a href="#critical-word-first">Critical Word First</a></li>
<li><a href="#early-restart">Early Restart</a></li>
<li><a href="#pipeline">Pipeline</a></li>
<li><a href="#multibank">Multibank</a></li>
<li><a href="#non-blocking">Non-blocking</a></li>
</ul>
</li>
<li><a href="#_8">参考资料</a></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Cache 是一个非常广泛、有深度的话题，无数的大牛在此道沉浸数十载，创造出了各种各样的优化技巧和实现技术，一篇短短的博文显然无法做到面面俱到，作为学习笔记，只争取说明白最基础的 Cache 知识。</p>
</div>
<h2 id="cache">为什么需要 Cache？</h2>
<h3 id="_1">面临的问题</h3>
<p>众所周知，处理器的集成度一直在按照摩尔定律逐渐提高，在 20 世纪的后 20 年内，core 的时钟频率也基本按照每 18 个月翻一番的速度增长，但是 DRAM 的时钟频率增长速度却每年只有 7%，所以两者的速度之间就形成了一个 <code>剪刀差</code>：随着时间的推移，这个差距会越来越大。即使在现在这个多核时代，core 的频率不再怎么提升了，这个差距仍然在扩大，因为核数量增加对内存的带宽需求也会相应增加。</p>
<p>这个问题是码农和硅农之间的矛盾：码农希望存储器的容量尽可能大的同时速度足够快，而实际上硅农受限于半导体技术，速度快的 SRAM 容量无法做得很大，成本太高；而容量大的 DRAM 速度无法做得很快。举个例子，假设一个 core 的频率为 2 GHz 的 4-way 超标量处理器，它直接访问 latency = 100ns 的 DRAM，那么访问一次 DRAM 的时间内处理器可以执行多少条指令呢？</p>
<div class="math">$$4*100*10^{-9}*2*10^9 = 800$$</div>
<p>这显然是不可接受的。</p>
<h3 id="_2">解决方案</h3>
<p>按照传统的冯 · 诺依曼结构，指令和数据都在内存中，CPU 只负责处理，内存和 CPU 关系就像是仓库和工厂，工厂加工的原料和生产出来的产品都要放在仓库中。但是摩尔定律和 “ 剪刀差 ” 导致工厂和仓库之间的运输能力成为整个系统的瓶颈，最简单直观的解决办法就是在工厂里面做一个小仓库，对应在处理器中就是存储器层次 <code>Memory Hierarchy</code>。</p>
<table>
<thead>
<tr>
<th>存储层次</th>
<th>大小</th>
<th>访问 latency</th>
<th>数据调度</th>
<th>调度单位</th>
<th>实现技术</th>
</tr>
</thead>
<tbody>
<tr>
<td>register file</td>
<td>&lt; 1 KB</td>
<td>0.25 ~ 0.5 ns</td>
<td>编译器</td>
<td>Word</td>
<td>CMOS 寄存器堆</td>
</tr>
<tr>
<td>cache</td>
<td>&lt; 16 MB</td>
<td>0.5 ~ 25 ns</td>
<td>硬件</td>
<td>block</td>
<td>CMOS SRAM</td>
</tr>
<tr>
<td>memory</td>
<td>&lt; 16 GB</td>
<td>80 ~ 250 ns</td>
<td>操作系统</td>
<td>page</td>
<td>CMOS DRAM</td>
</tr>
<tr>
<td>I/O device</td>
<td>&gt; 100 GB</td>
<td>5ms</td>
<td>人工</td>
<td>file</td>
<td>disk</td>
</tr>
</tbody>
</table>
<p>cache 能缓解问题的原因在于程序具有<strong>局部性原理</strong>：</p>
<ul>
<li><code>时间局部性</code>：一个数据被访问之后，短期内很大概率会被再次访问</li>
<li><code>空间局部性</code>：一个数据被访问之后，短期内很大概率会访问相邻数据</li>
<li><code>算法局部性</code>：程序时空间隔都很大地重复访问数据，常见于图像处理等算法</li>
</ul>
<p>其中常规的 prefetch 算法无法识别并利用算法局部性，只能靠程序员观察数据访问规律，并重写算法，依靠 programmer/ccompiler 插入 software prefetch 指令。</p>
<p>Cache 的出现可以说是一种无奈的妥协。如果 DRAM 的速度足够快，或者 SRAM 的容量可以做到足够大，我们的烦恼不复存在了，cache 也没有存在的必要了。但是在未来一段时间内，当今硅工艺不发生革命性变化的前提下，这是很难实现的一件事情，所以 cache 是有必要的。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>冯诺依曼结构中有个永恒的主题：如何喂饱饥饿的 CPU，即如何提供稳定的指令流和数据流：</p>
<ul>
<li>指令流：分支预测</li>
<li>数据流：cache</li>
</ul>
<p>经过多年发展，大概能达到半饱的程度：4 发射的结构，IPC = 2 就已经很不错了。</p>
</div>
<h2 id="cache_1">如何确定 Cache 规格？</h2>
<p>cache 的规格可以总结为下面几个问题。</p>
<h3 id="_3">容量选择</h3>
<p><code>Q：Cache 的总容量应该设置为多少合适？</code></p>
<p>A：没有标准答案，应该根据 <code>应用需求</code> 和 <code>微架构</code> 设计特点做出选择。</p>
<p>首先，cache 容量对 area 有最直接的影响，一般 cache 占用处理器 60% ~ 80% 的晶体管和 30% 以上的总面积，在某些处理器中甚至达到了 80% 的面积。所以确定 cache 容量时，首先要考虑的就是面积约束（即 area 和 money）。</p>
<p>其次，在满足面积约束的前提下，显然希望 cache 性能越高越好。根据 3C 模型，可以知道</p>
<ul>
<li>增加容量的优点：可以降低 capacity miss，从而降低整体的 miss rate</li>
<li>增加容量的缺点：导致时序变差，导致 hit time 和 miss penality 变大</li>
</ul>
<p>所以 cache 容量是一把双刃剑，并不是越大性能就越高。</p>
<p>目前的主流方案是多级 cache，不同级别的 cache 设计目标不同，所以容量规格也不同。因为其他级 cache 的出现，每一级 cache 的最佳规格、设计思路与单级 cache 方案完全不同：</p>
<ul>
<li>L1 离 core 最近，其目标是跟上 core 的速度，所以会选择小容量、低相联度的结构，牺牲一些 hit rate，尽量减小 latency，换取高 throughput 和低 hit time。它的容量和 block size 相比于单级 cache 来说都要小很多，以减小 miss penality</li>
<li>L2 离 core 远一些，其目标则是低 miss rate，所以会选择大容量、高相联度的结构，牺牲一些频率、throughput 和 latency，换取更低的 miss rate。它的容量和 block size 都要比单级 cache 要大很多</li>
</ul>
<h3 id="block">Block 大小选择</h3>
<p><code>Q：每个 block 的大小应该设置为多少？</code></p>
<p>A：应该根据 <code>cache size</code> 做出选择 , 一般为 4～8 Word，常见的组合有 cache size = 4KB，block size = 32B; cache size &gt; 64KB, block size = 64B。</p>
<p>较大的 block 可以更好地利用空间局部性，所以可以降低 miss rate，但是当 block 占 cache 容量的比例大到一定程度时，因为 block 的数量变得很少，此时会有大量的冲突，数据在被再次访问前就已经被替换出去了，而且太大的 block 内部数据的空间局部性也会降低，所以会导致 miss rate 反而上升。</p>
<p>随着 block 的增大，miss rate 的改善逐渐降低，但是在不改变 memory 系统的前提下，miss penalty 会随着 block 的增大而增大，所以当 miss penalty 超过了 miss rate 的收益，cache 的性能就会变低。</p>
<p>block 的大小还依赖于下一级存储器的 latency 和 throughput：</p>
<ul>
<li>latency 和 throughput 越大，越应该使用大 block：因为每次 miss 可以取得更多的数据，但是 miss penality 增长很小（因为此时 miss penality 的主要成分是 latency，所以增大 block 额外传输数据的时间占比很小）</li>
<li>latency 和 throughput 越小，越应该使用小 block：因为这种情况下增大 block 并不会节省多少时间（比如小块的 penlaty*2 和一个两倍大小 block 的 penalty 相同，此时显然选小 block 更好，还能减小 conflict miss）</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>较大 block 会导致较长的传输时间，虽然这部分时间很难优化，但是我们可以隐藏一些数据传输的时间，从而降低 miss penalty。实现这个效果的最简单的技术叫做 <code>early restart</code>：一旦接收到需要的 word 就立即就开始重启流水线，而不是等到整个 block 都返回后才重启。许多处理器都在 I-cache 上使用这个技术，效果甚佳，这是因为大部分指令访问都具有连续性。这个技术对于 D-cache 来说效果就没那么好了，因为数据访问的预测性没那么好，在传输结束前请求另外一个 block 中 word 的概率很高，而此时前一次请求的数据传输还没有结束，所以仍然会导致处理器 stall。</p>
<p>还有一种更加复杂的机制叫做 <code>requested word first</code> 或者是 <code>critical word first</code>，这种方案会重新组织 memory 的结构，使得被请求的 word 优先返回，然后按照顺序返回后续数据，最后反卷到 block 的开头部分。这种方法比 early restart 稍微快一点，但是会受到相同的限制。</p>
</div>
<h3 id="_4">映射方式</h3>
<p><code>Q：应该如何组织 cache 的存储结构？</code></p>
<p>A：根据 <code>cache size</code> 三选一，有个一般性的规律：</p>
<p><strong>2:1 cache rule of thumb</strong>：容量为 N、直接映射的 miss rate = 容量为 N/2、相联度为 2-way 的 miss rate。</p>
<p>cache 的工作方式和停车场非常类似，如果停车场（cache）中有可用的空车位（cache line），那么汽车（data）就可以停在该车位中；如果停车场已经没有空车位，那么就要先把某个车开出来（数据替换出去），然后才能把新来的车停进去。而在停车场找车时，如果停车场很大，而且所有的车都随机停，那么找车（查找数据）的速度就会很慢。</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>类比</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>直接映射</td>
<td>固定车位</td>
<td>硬件简单、成本低，查找速度快</td>
<td>不灵活、易冲突、利用率低</td>
</tr>
<tr>
<td>全相联</td>
<td>随机车位</td>
<td>冲突小、利用率高</td>
<td>硬件复杂，查找速度慢</td>
</tr>
<tr>
<td>组相联</td>
<td>区域内随机车位</td>
<td>折中</td>
<td>折中</td>
</tr>
</tbody>
</table>
<p>组相联是另外两种方式的折中：组之间是直接映射、组内是全相联。直接映射可以看作是组数 set = full 的特例，全相联可以看作是 set = 1 的特例。</p>
<h3 id="_5">替换策略</h3>
<p><code>Q：如果一个 set 中没有可用的 way 时，应该把哪个 way 替换出去？</code></p>
<p>A：根据 <code>associative</code> 和实现复杂度三选一</p>
<ul>
<li><code>LRU</code> (Least Recently Used)</li>
<li><code>random</code></li>
<li><code>FIFO</code></li>
</ul>
<p>根据理论分析，应该把最不活跃的数据替换出去，因为它再次被用的概率最小，即 LRU 策略。但是 LRU 的实现代价比较高，一般超过 8 way 就不可接受了，所以常用方法是 pseudo-LRU，用较小的代价实现近似 LRU 的效果。另外两种则很直观。根据《量化分析》的统计结果，在小容量时 LRU 的效果最好，当容量变大后，LRU 和 random 的效果差不多，FIFO 的效果则取决于具体程序。</p>
<h3 id="_6">写回策略</h3>
<p><code>Q：cache 应该如何处理写回数据？</code></p>
<p>A： 实际需求和数据访问模式，还有其他约束条件共同决定</p>
<p>常见的组合方式：</p>
<table>
<thead>
<tr>
<th></th>
<th>hit</th>
<th>miss</th>
</tr>
</thead>
<tbody>
<tr>
<td>方式一</td>
<td>write back</td>
<td>write allocate</td>
</tr>
<tr>
<td>方式二</td>
<td>write through</td>
<td>write non-allocate</td>
</tr>
</tbody>
</table>
<p>hit 下两种不同处理方式的对比：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>优点</th>
<th>缺点</th>
<th>应用</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>write through</code></td>
<td>硬件简单、flush 代价小</td>
<td>性能差</td>
<td>L1</td>
</tr>
<tr>
<td><code>write back</code></td>
<td>hit 效率高、带宽利用率高</td>
<td>硬件复杂、flush 代价高</td>
<td>L2 之后</td>
</tr>
</tbody>
</table>
<p>下面分类讨论不同策略的含义和优缺点。</p>
<h4 id="write-throughwt-write-allocate">Write-Through(WT) + write allocate</h4>
<p><strong>定义：</strong> hit 时同时写入 cache memory 和 lower memory；miss 时 allocate，先读回后写入</p>
<p><strong>优点：</strong> 兼顾了 fast retrieval 和 data lost risk，维护一致性简单</p>
<p><strong>缺点：</strong> 只加速 read；write latency 差</p>
<p><strong>适用场景：</strong> write once，retrieval frequently 的场景（偶尔个别 write latency 可忍受）</p>
<h4 id="write-backwb-write-allocate">Write-Back(WB) + write allocate</h4>
<p><strong>定义：</strong> hit 时写入 cache memory，不写入 lower memory；miss 时 allocate，先读回后写入</p>
<p><strong>优点：</strong> 同时加速 read/write；兼顾了 fast retrieval 和 write latency</p>
<p><strong>缺点：</strong> 有 data lost risk，维护一致性困难</p>
<p><strong>适用场景：</strong> read-write 混合场景</p>
<h4 id="write-aroundwa">Write-Around(WA)</h4>
<p><strong>定义：</strong> write through + write non-allocate</p>
<p><strong>优点：</strong> 避免 pollution（一次性数据不会 flood），无 data lost risk，维护一致性简单</p>
<p><strong>缺点：</strong> 只加速 read；write latency 差</p>
<p><strong>适用场景：</strong> 不频繁 retrieval write data；stream 应用</p>
<h4 id="write-invalidwi">Write-Invalid(WI)</h4>
<p><strong>定义：</strong> 只写入 lower memory，write hit 时 invalid 命中行</p>
<p><strong>优点：</strong> 确保 read 性能，无 data lost risk，维护一致性简单</p>
<p><strong>缺点：</strong> 只加速 read；write latency 差</p>
<p><strong>适用场景：</strong> read intensive 应用</p>
<h4 id="write-onlywo">Write-Only(WO)</h4>
<p><strong>定义：</strong> write 同 write back，read 不会存储到 cache memory</p>
<p><strong>优点：</strong> 确保 write 性能</p>
<p><strong>缺点：</strong> 只加速 write；有 data lost risk，维护一致性困难</p>
<p><strong>适用场景：</strong> write intensive 应用</p>
<h4 id="pass-throughpt">Pass-Through(PT)</h4>
<p><strong>定义：</strong> bypass 所有请求到 lower memory</p>
<p><strong>优点：</strong> N/A</p>
<p><strong>缺点：</strong> N/A</p>
<p><strong>适用场景：</strong> debug</p>
<h4 id="summary">Summary</h4>
<table>
<thead>
<tr>
<th>policy</th>
<th>speedup</th>
<th>retrieval</th>
<th>write latency</th>
<th>consistency</th>
<th>data lost risk</th>
<th>speical</th>
</tr>
</thead>
<tbody>
<tr>
<td>WT + WA</td>
<td>RO</td>
<td>+</td>
<td></td>
<td>+</td>
<td>+</td>
<td></td>
</tr>
<tr>
<td>WB + WA</td>
<td>RW</td>
<td>+</td>
<td>+</td>
<td></td>
<td></td>
<td>RW mix</td>
</tr>
<tr>
<td>WA</td>
<td>RO</td>
<td>+(RO)</td>
<td></td>
<td>+</td>
<td>+</td>
<td>Stream</td>
</tr>
<tr>
<td>WI</td>
<td>RO</td>
<td>+(RO)</td>
<td></td>
<td>+</td>
<td>+</td>
<td>Read intensive</td>
</tr>
<tr>
<td>WO</td>
<td>WO</td>
<td>+(WO)</td>
<td>+</td>
<td></td>
<td></td>
<td>Write intensive</td>
</tr>
<tr>
<td>PT</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>debug</td>
</tr>
</tbody>
</table>
<h3 id="arm-m-cache">实例：Arm M 系列 cache 规格</h3>
<p><a href="https://en.wikipedia.org/wiki/ARM-Cortex-M">ARM M 系列配置</a></p>
<p>总结可以得到下面规律：</p>
<ul>
<li>Cortex-M 系列定位为 MCU，主要应用于嵌入式领域</li>
<li>中低端 core 内部没有集成任何类型的 cache（M0, M0+, M3, M4）</li>
<li>在系统层次，可以为 core 配置系统级别的 cache/TCM</li>
<li>高端 core 内部可能同时集成了 cache 和 TCM（M7）</li>
<li>STM32F7 中集成的 M7 是一款双发射、6-stage 的超标量嵌入式处理器，core 内部同时配置了 cache 和 TCM</li>
<li>I-cache 容量为 0~64KB，cache block 大小为 32B，2-way</li>
<li>D-cache 容量为 0~64KB，cache block 大小为 32B，4-way</li>
</ul>
<h3 id="arm-a-cache">实例：Arm A 系列 cache 规格</h3>
<p><a href="https://en.wikipedia.org/wiki/List-of-ARM-microarchitectures#Designed-by-ARM">ARM A 系列配置</a></p>
<p>总结可以得到下面规律：</p>
<ul>
<li>Cortex-A 系列定位移动端和用户级应用</li>
<li>A 系列要支持 OS，所以全系配置了不同大小的 cache，舍弃了 TCM</li>
<li>低性能 core 只集成了 L1 cache，高性能 core 还集成了 L2 cache，甚至是 L3</li>
</ul>
<h3 id="t-head-c906">实例：T-head C906</h3>
<p>TODO</p>
<h2 id="cache_2">Cache 的性能指标</h2>
<p><code>Q：如何评价一个 cache 的性能？</code></p>
<h3 id="amat">AMAT</h3>
<p>Cache 最常用的性能指标是： <code>AMAT</code>(Average memory aceess time) ，显然 Cache 系统设计越合理，对 core 表现出来的性能越好，AMAT 就越小。根据定义可以知道 AMAT 的计算公式如下：</p>
<p><span class="math">\(AMAT = Time\ for\ a\ hit + Miss\ rate * Miss\ penalty\)</span></p>
<p>对于多级 Cache 系统，AMAT 公式如下（以两级 Cache 为例）：</p>
<p><span class="math">\(T_{avg}=H_1*C_1 + (1-H_1)*(H_2*(C_1 + C_2) + (1-H_2)*(C_1 + C_2 + M)\)</span></p>
<p>每个符号的含义：</p>
<ul>
<li><span class="math">\(H_1\)</span> 表示 L1 cache 的命中率</li>
<li><span class="math">\(H_2\)</span> 表示 L2 cache 的命中率</li>
<li><span class="math">\(C_1\)</span> 表示 L1 cache 命中访问时间</li>
<li><span class="math">\(C_2\)</span> 表示 L2 cache 命中访问时间（即 L1 miss 但是 L2 hit 的 penalty）</li>
<li><span class="math">\(M\)</span> 表示 DDR 的访问时间（即 L2 miss 的 penalty）</li>
</ul>
<p>也可以换一种算法：</p>
<p><span class="math">\(T_{avg}= C_1 + (1-H_1)*C_2 + (1-H_1)*(1-H_2)*M\)</span></p>
<p>可以证明两种方式是等价的。</p>
<h3 id="_7">其他指标</h3>
<p>AMAT 是 dcache 对外表现出来的综合性指标，如果想评估深入评估 dcache 微架构，那么应该用更多的细分指标来衡量。</p>
<p>最基本的指标是所有硬件都适用的 <code>throughput</code> 和 <code>latency</code>，描述 cache 响应命令的吞吐率和时延。</p>
<p>Cache 特有的最简单的指标是命中率 <code>hit ratio</code>，描述 cache 规格 / 微架构是否合理，合理的设计命中率都比较高，95% 以上。</p>
<p>另外如果想衡量 prefetch 的性能，主要有两个指标 <code>coverage</code> 和 <code>accuracy</code>，</p>
<div class="math">$$coverage=\frac{miss\ eliminated\ by\ prefetch}{total\ miss\ without\ prefetch}$$</div>
<div class="math">$$accuracy=\frac{miss\ eliminated\ by\ prefetch}{total\ prefetch}$$</div>
<p>coverage 描述了漏警性能，accuracy 描述了虚警性能，一般这两者很难兼得。同理，victim cache 也可以用类似的指标来衡量。</p>
<h2 id="cache_3">Cache 的性能分析模型</h2>
<p><strong>3C 模型</strong>，也就是分析清楚 cache 的 miss 可以分为几类，每一类的产生原因是什么：</p>
<ul>
<li><code>compulsory miss</code></li>
<li><code>capacity miss</code></li>
<li><code>conflict miss</code></li>
</ul>
<p>一旦知道了 miss 产生的原因，也就可以针对性地采用各种方法来降低。虽然按照 3C 模型来分析时，优化某个因素的行为可能会导致另外一个因素的恶化，但是总体上它仍然是个很有用的工具，可以帮助我们在做设计时对 cache 性能进行建模（而且目前我们也没有更好的模型）。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>当系统中有多个 cache 时，会额外再增加一个 C <code>coherency miss</code>，即为了保持一致性导致 cache 进行 flush，产生 miss。此时就是 4C 模型。</p>
</div>
<h2 id="cache_4">Cache 性能优化方法</h2>
<p>有了 cache 模型，就可以根据模型来优化性能，针对性能公式中的每个因子，优化思路可以分为下面几类：</p>
<table>
<thead>
<tr>
<th>优化思路</th>
<th>优化方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>减小 hit time</td>
<td>小而简单的 L1 cache、路预测、Vitural Index/Physical Tag、serial 访问</td>
</tr>
<tr>
<td>减小 miss rate</td>
<td>增大容量、增大 block size、增加关联度、预取、软件优化</td>
</tr>
<tr>
<td>减小 miss penalty</td>
<td>victim cache、write buffer、多级 cache、关键字优先、写合并</td>
</tr>
<tr>
<td>提高 throughput</td>
<td>pipeline、multibank( 多端口 )、非阻塞</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>miss penality 本质是就是一块数据的搬运耗时，可以借用 “ 火车过山洞 ” 模型：</p>
<p>假设火车长度为 l，火车的时速为 v，火车头进入山洞的时刻为 t1，火车头出山洞的时间为 t2，火车尾出山洞的时刻为 t3，那么可以将火车过山洞的过程和数据搬运过程对应起来</p>
<table>
<thead>
<tr>
<th>火车过山洞</th>
<th>搬运数据</th>
</tr>
</thead>
<tbody>
<tr>
<td>t2-t1</td>
<td>latency</td>
</tr>
<tr>
<td>v</td>
<td>throughput</td>
</tr>
<tr>
<td>l</td>
<td>data-amount</td>
</tr>
<tr>
<td>t3-t2 = l/v</td>
<td>transfer-time = data-amount/throughput</td>
</tr>
</tbody>
</table>
<p>火车过山洞的总时间 = t3 - t1 = (t3-t2) + (t2-t1)</p>
<p>数据传输的总时间 = transfer-time + latency</p>
<p>所以优化 miss penality 的方法主要就是下面这 3 种：</p>
<ul>
<li>减小 latency，这个取决于 hierarchy 中的下一级， cache 很难改变</li>
<li>减小 data-amout，每次 miss 时少取一些数据</li>
<li>增加 throughput，提高 cache 和 hierarchy 下一级之间的传输带宽</li>
</ul>
</div>
<h3 id="small-and-simple-l1-cache">Small and Simple L1 Cache</h3>
<p>面临的问题：复杂 L1 的速度很难跟上 core 的时钟频率。</p>
<p>解决思路：思路 1（减小 hit time），简化硬件设计、减小 size 和 associativity，从而减小 hit time。</p>
<p>付出的代价：miss rate 增加，需要下级 cache 作为补充。</p>
<h3 id="way-prediction">Way Prediction</h3>
<p>面临的问题：对于并行访问 tag 和 data 的组相联 cache，必须经过下面 3 个步骤，</p>
<ol>
<li>读 tag memory</li>
<li>比较 tag 内容</li>
<li>根据读 tag 的结果选中 &amp; 操作 data memory（mux + 写 data memory）</li>
</ol>
<p>整个过程组合逻辑很长，hit time 较长，所以时钟频率无法做到很高。</p>
<p>解决思路：思路 1（减小 hit time），采用 prediction bits，把组相联的 cache 当初直接相联来用，那么第二步的比较 tag 就只有 1 个 way 做比较，如果命中则相当于直接相联 cache 结构；否则在下个周期检查剩余 way 的 tag。因为这种方式第二步的组合逻辑变少，hit time 就变短了。</p>
<p>付出的代价：使得 pipeline 难以实现。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>还有一种更进一步的做法叫做 <code>way selection</code>，即第一步也只读 1 个 way 的 tag，如果 miss 则需要重新读剩余 tag、作比较、操作 data memory。这种方法显然可以更省功耗，但是缺点就是一旦 miss，付出的代价很大，因为要完成重新执行一遍步骤 123。</p>
</div>
<h3 id="vitural-indexphysical-tag">Vitural Index/Physical Tag</h3>
<p>面临的问题：全部使用虚拟地址时，必须经过一道查询 TLB 的过程，增加了整体的访问 latency。</p>
<p>解决思路：思路 1（减小 hit time），采用 page offset 作为 cache tag，省去查询 TLB 的过程。</p>
<p>付出的代价：实现复杂度增加，功耗增加。</p>
<h3 id="serial">Serial 访问</h3>
<ul>
<li>parrel：tag array 和 data array 并行访问，根据 tag array 的读结果产生 mux 信号，从 data array 的结果中选出目标 way</li>
<li>serial：先访问 tag array，根据结果产生 data array 的片选信号，直接读出目标 way</li>
</ul>
<p>面临的问题：parrel 方式 critial path 较长，频率低</p>
<p>解决思路：思路 1（减小 hit time），采用 serial 方式优化 critical path，同时因为省去了不必要的读 data array，所以功耗也更低。</p>
<h3 id="cache-size">Cache Size</h3>
<p>面临的问题：cache 容量不够，数据频繁被替换。</p>
<p>解决思路：思路 2（降低 miss rate），降低 capacity miss</p>
<p>付出的代价：导致 hit time 变大，同时成本和功耗也会变高。</p>
<h3 id="block-size">Block Size</h3>
<p>面临的问题：block 太小，对空间局部性的利用不充分。</p>
<p>解决思路：思路 2（降低 miss rate），可以充分利用空间局部性，降低 compulsory miss。而且扩大 block size 会导致 tag 位宽变小，相应地可以减小一点功耗。</p>
<p>付出的代价：导致 miss penality 变大；而且过大的 blocksize 会适得其反。</p>
<h3 id="set-associativity">Set Associativity</h3>
<p>面临的问题：关联度太小，一个 set 内频繁竞争。</p>
<p>解决思路：思路 2（降低 miss rate），降低 conflict miss。</p>
<p>付出的代价：导致 hit time 变大，同时成本的功耗也会变高。</p>
<h3 id="prefetch">Prefetch</h3>
<p>面临的问题：如果每次发生 miss 时只取回当前 cache line，那么 cache 向 DDR 发送的 burst len 和 outstanding 都很小，效率很低。频繁发生 compulsory miss。</p>
<p>解决思路：思路 2（降低 miss rate）。在取回当前 cache line 的同时以大 burst len 和 outstanding 高效地多取一些相邻数据，这样访问这些预取数据时就不会发生 miss。</p>
<ul>
<li>软件预取：有些 ISA 定义了预取指令，程序员可以通过软件进行预取<ul>
<li>register prefetch：把数据预取到 register 中</li>
<li>cache prefetch：把数据预取到 cache 中</li>
</ul>
</li>
<li>硬件预取：cache 自主可以观测 unit-stride 和 stride 的规律，自动预取数据</li>
</ul>
<table>
<thead>
<tr>
<th>方案</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>OBL</code> (one block look-ahead)</td>
<td>每次多预取一个 cache block</td>
</tr>
<tr>
<td><code>stream buffer</code></td>
<td>多预取的数据存储在 stream buffer 中，miss 时再写入 cache line</td>
</tr>
<tr>
<td><code>SPT</code> (stride predict table)</td>
<td>硬件检测 load 是否存在 stride 模式，取回数据直接写入 cache line</td>
</tr>
<tr>
<td><code>stream cache</code></td>
<td>结合 steam buffer 和 SPT，把预取回来的数据放在一个小 cache 中</td>
</tr>
</tbody>
</table>
<p>预取数据不直接存到 cache 中的原因是避免 “cache 污染 ”，但是 stream buffer 不灵活，所以改进方案是把预取数据放到一个 stream cache 中。</p>
<p>付出的代价：硬件复杂度增加，消耗更多资源。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>prefetch 有效的前提是有剩余带宽未被利用。如果 prefetch 干扰了正常 miss 的读取，那么反而会降低性能。</p>
</div>
<h3 id="software-optimize">Software Optimize</h3>
<p>纯软件，不需要改任何硬件。</p>
<p>同样也可以分为两类：改善 miss rate 或者是改善 miss penality。</p>
<ul>
<li>loop interchange：交换嵌套 loop 的顺序</li>
<li>bloking：对数据分块处理</li>
</ul>
<h3 id="vicitm-cache">Vicitm Cache</h3>
<p>面临的问题：conflict miss 导致频繁的读写下一级 memory，导致整体性能降低。增加相联度代价太大，其他 set 没有这个需求。</p>
<p>解决思路：思路 2（降低 miss rate），另外增加一个小容量（通常 4~16 个数据）、全相联的 cache，缓存被替换出来的数据。一般和 main cache 为 exclusive 关系。</p>
<p>和 Victim Cache 相对应的还有一种 Filter Cache，即在数据进入 main cache 前，先写入 Filter cache，等数据再次被使用时才写入 main cache，用来过滤偶然数据，提高整体利用率。</p>
<p>付出的代价：硬件复杂度增加。维护 victim cache 和 main cache 之间的一致性。</p>
<h3 id="write-buffer">Write Buffer( 读优先 )</h3>
<p>面临的问题：如果发生 miss 时被替换的 block 为 dirty，则必须先将其写回下级 memory 后才能把目标 block 读进来，整个过程是串行的。当写下级的代价很高时，会导致 miss penality 很大。</p>
<p>解决思路：思路 3（减小 miss penality），先将 dirty block 写入一本本地的 write buffer，为目标 block 尽早腾出空间。等下级 memory 空闲时，再将 dirty block 写入其中。</p>
<ul>
<li>对于 write-back 的 cache，就是把 dirty cache line 整条都写入 write buffer</li>
<li>对于 write-through 的 cahe，就是把 dirty data 写入 write buffer</li>
</ul>
<p>L1 D-cache 通常采用 write-through 方案，配合 write buffer 提高性能。</p>
<p>付出的代价：硬件复杂度增加。cache 发生 miss 时首先要查询 write buffer（需要 CAM）</p>
<h3 id="write-merging">Write Merging</h3>
<p>面临的问题：如果 core 每次只写一个 word，那么普通 write buffer 的每个 entry 的大部分空间都会被浪费掉，write buffer 很容易达到 full。</p>
<p>解决思路：思路 3（减小 miss penality），每次把数据写入 write buffer 时，检查是否可以合并到已有 entry 中。</p>
<p>付出的代价：硬件复杂度增加。</p>
<h3 id="multiple-level">Multiple Level</h3>
<p>面临的问题：单级 cache 无法同时满足 fast hit 和 few miss 的需求。</p>
<p>解决思路：思路 3（减小 miss penality），存储器层次结构 L1 + L2 + L3。一般 L1/L2 为每个 core 私有，L2/L3 共享。</p>
<ul>
<li>L1： 小容量、低关联度、write-through</li>
<li>L2/L3: 大容量、高关联度、write-back</li>
</ul>
<p>付出的代价：面积变大、解决一致性问题、硬件复杂度增加。</p>
<h3 id="critical-word-first">Critical Word First</h3>
<p>面临的问题：core 每次访问实际上只需要一个 word，但是 cache miss 时取要取回整个 block，耗时较长。</p>
<p>解决思路：思路 3（减小 miss penality），采用 “ 不耐心 ” 的做法，优先向下级 memory 请求 miss 的 word，一旦读回来立即返回给 core。</p>
<p>付出的代价：硬件控制复杂化。</p>
<h3 id="early-restart">Early Restart</h3>
<p>面临的问题：同 Critical word first</p>
<p>解决思路：思路 3（减小 miss penality），另外一种 “ 不耐心 ” 的做法，按照正常顺序请求数据，但是取回数据后立即把 word 发给 core。</p>
<p>付出的代价：硬件控制复杂化。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>critical word first 和 early restart 只有在 block size 很大时收益才比较明显。</p>
</div>
<h3 id="pipeline">Pipeline</h3>
<p>面临的问题：读 D-cache 时 tag memory 和 data memory 可以并行同时读；但是对于写 D-cache，必须经过 way prediction 小节中提到的 3 个过程。整个过程串行操作时钟频率会很低，一般的做法是分为两个 cycle，第一拍读 tag 作比较，第二拍写 data memory。此时 throughput 为 0.5 instr/cycle。</p>
<p>解决思路：思路 4（提高 throughput），将整个过程 pipeline 化，达到 1 instr/cycle 的 throughput。</p>
<p>付出的代价：硬件复杂度增加。后续指令要额外检查 pipeline 上的数据，增加 forward 通路。显然 pipeline 越深，mispredict 时 flush 的代价就越大，同时 load-to-use 的 latency 也越大。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>pipeline 主要应用在 L1 上，因为它的访问带宽会限制 instruction throughput。目前大多数 core 的 L1 都采用 3~4 级 pipeline 的方式。</p>
</div>
<h3 id="multibank">Multibank</h3>
<p>面临的问题：单 bank 的最高 throughput = 1 instr/cycle，无法满足超标量处理器的需求</p>
<p>解决思路：思路 4（提高 throughput）。多 bank（多端口）有几种常见方案：</p>
<p>一般多端口的实现方案有以下几张：</p>
<table>
<thead>
<tr>
<th>实现方案</th>
<th>含义</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>true multiple port</code></td>
<td>真正的多端口，memory 有相应数量的读写端口</td>
<td>实现代价太大，不可接受</td>
</tr>
<tr>
<td><code>virtual multiple port</code></td>
<td>memory 仍然是单端口，cache 频率是 core 的倍数</td>
<td>可扩展性差，现在不可接受</td>
</tr>
<tr>
<td><code>copy multiple port</code></td>
<td>memory 仍然是单端口，但是复制多份，保持 copy 之间的同步</td>
<td>浪费资源，同步控制复杂</td>
</tr>
<tr>
<td><code>multiple bank</code></td>
<td>memory 仍然是单端口，按照地址分 bank 交织</td>
<td>折中方案，普遍应用</td>
</tr>
</tbody>
</table>
<p>分 bank 按照地址交织，也有一些缺点，比如要依靠编译器降低冲突概率；发生冲突时性能变差；内部 crossbar 对 PR 不友好，但是相比于其他几个方案，代价最小，应用最广泛。</p>
<p>付出的代价：增加硬件复杂度，消耗更多资源。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>multibank 应用在 L1 时主要为了提供高 throughput，应用于 L2, L3 时更主要的目标是做功耗控制。</p>
</div>
<h3 id="non-blocking">Non-blocking</h3>
<p>面临的问题：miss 会阻塞 core 的流水线，效率低。</p>
<p>解决思路：思路 4（提高 throughput），cache 在处理当前 miss 的同时处理后续的请求。</p>
<p>付出的代价：增加硬件复杂度，消耗更多资源（MSHR）。</p>
<p><code>in-cache MSHR</code>：给 tag array 新增额外的 1bit transient 信号，如果某 entry 处于 transient mode，tag array 保存 base address，data array 保存所有的 MSHR 信息，具体形式既可以是 implicit 也可以是 explicit。</p>
<table>
<thead>
<tr>
<th>MSHR 实现方式</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>implicitly</td>
<td>消耗资源少（不需要记录 offset）</td>
<td>一个 word 只能 miss 一次、深度固定</td>
</tr>
<tr>
<td>explicitly</td>
<td>同一个 word 可以 miss 多次、深度灵活可配</td>
<td>消耗资源多（需要额外记录 offset）</td>
</tr>
<tr>
<td>in-cache</td>
<td>不需要任何额外资源保存 MSHR</td>
<td>处理步骤增多，latency 变大</td>
</tr>
</tbody>
</table>
<h2 id="_8">参考资料</h2>
<p>Computer Organization and Design RISC-V Edition. David A. Patterson, John L. Hennessy</p>
<p>Computer Architecture: A Quantitative Approach. John L. Hennessy, David A. Patterson</p>
<p>Processor Microarchitecture: An Implementation Perspective. Antonio Gonzalez</p>
<p>《计算机体系结构》 胡伟武</p>
<p>《超标量处理器》 姚永斌</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'blue ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>


  <div class="neighbors">
    <a class="btn float-left" href="https://qian-gu.github.io/posts/tools/fusesoc-summary.html" title="Fusesoc 小结">
      <i class="fa fa-angle-left"></i> Previous Post
    </a>
    <a class="btn float-right" href="https://qian-gu.github.io/posts/tools/iverilog-verilator-and-gtkwave.html" title="开源 IC 工具/库 —— iverilog、verilator 和 gtkwave">
      Next Post <i class="fa fa-angle-right"></i>
    </a>
  </div>


    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <ins class="adsbygoogle ads-responsive"
         data-ad-client="ca-pub-1821536199377100"
         data-ad-slot="4843941849"></ins>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
    </script>

<!-- Gitalk -->
<div id="gitalk-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="https://qian-gu.github.io/theme/gitalk/md5.min.js"></script>
<script>
var gitalk = new Gitalk({
  clientID: '4b3de26a6e80be727416',
  clientSecret: '993d79339c842fc56d9739ef268f38806dc93f50',
  repo: 'qian-gu.github.io',
  owner: 'qian-gu',
  admin: ['qian-gu'],
  id: md5(location.href),
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- End Gitalk -->
</article>

<footer>
<p>
  &copy; 2025  - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
  <span class="footer-separator">|</span>
  Switch to the <a href="javascript:void(0)" onclick="theme.switch(`dark`)">dark</a> | <a href="javascript:void(0)" onclick="theme.switch(`light`)">light</a> | <a href="javascript:void(0)" onclick="theme.switch(`browser`)">browser</a> theme
  <script id="dark-theme-script"
          src="https://qian-gu.github.io/theme/dark-theme/dark-theme.min.js"
          data-enable-auto-detect-theme="True"
          data-default-theme="ligtht"
          type="text/javascript">
  </script>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
           src="https://qian-gu.github.io/theme/img/cc/by-sa.png"
         width="80"
         height="15"/>
  </a>
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Qian's Blog ",
  "url" : "https://qian-gu.github.io",
  "image": "https://qian-gu.github.io/images/logo.png",
  "description": "Qian's Thoughts and Writings"
}
</script><a href="https://github.com/qian-gu/qian-gu.github.io" target="_blank" class="github-corner" aria-label="View source on Github">
    <svg width="80"
         height="80"
         viewBox="0 0 250 250"
         style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;"
         aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor"
              style="transform-origin: 130px 106px;"
              class="octo-arm">
        </path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor"
              class="octo-body">
        </path>
    </svg>
</a>
  <script>
    window.loadStorkIndex = function () {
      stork.initialize("https://qian-gu.github.io/theme/stork/stork.wasm")
      stork.register("sitesearch", "https://qian-gu.github.io/search-index.st", { showProgress: false });
    }
  </script>
  <script src="https://qian-gu.github.io/theme/stork/stork.js"></script>

</body>
</html>